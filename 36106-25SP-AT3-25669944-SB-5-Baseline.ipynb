{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ6wc2HE0pke"
      },
      "source": [
        "# **Baseline Notebook**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFVpE17Ahezu"
      },
      "source": [
        "---\n",
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "!pip install -q utstd\n",
        "\n",
        "from utstd.folders import *\n",
        "from utstd.ipyrenders import *\n",
        "\n",
        "at = AtFolder(\n",
        "    course_code=36106,\n",
        "    assignment=\"AT3\",\n",
        ")\n",
        "at.run()\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')"
      ],
      "metadata": {
        "id": "S8jFaNXqvV5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Student Information"
      ],
      "metadata": {
        "id": "Bv9hJ8IO8MTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "group_name = \"Group 12\"\n",
        "student_name = \"Victor Rono\"\n",
        "student_id = \"25669944\""
      ],
      "metadata": {
        "id": "ot2igNok8NbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h1\", key='group_name', value=group_name)"
      ],
      "metadata": {
        "id": "nMGPhFYunsRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h1\", key='student_name', value=student_name)"
      ],
      "metadata": {
        "id": "NE3R8WY98N_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h1\", key='student_id', value=student_id)"
      ],
      "metadata": {
        "id": "anavsHjV8OM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 0. Python Packages"
      ],
      "metadata": {
        "id": "Is0ghwXODHkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.a Install Additional Packages\n",
        "\n",
        "> If you are using additional packages, you need to install them here using the command: `! pip install <package_name>`"
      ],
      "metadata": {
        "id": "CgTrMfyylVLf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D79tb2V-lVpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.b Import Packages"
      ],
      "metadata": {
        "id": "mXFKfa2tp1ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import altair as alt"
      ],
      "metadata": {
        "id": "GBEAwdncnlAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8MNBrC4Zgz6"
      },
      "source": [
        "---\n",
        "## A. Assess Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "# Load data\n",
        "try:\n",
        "  X_train = pd.read_csv(at.folder_path / 'X_train.csv')\n",
        "  y_train = pd.read_csv(at.folder_path / 'y_train.csv')\n",
        "\n",
        "  X_val = pd.read_csv(at.folder_path / 'X_val.csv')\n",
        "  y_val = pd.read_csv(at.folder_path / 'y_val.csv')\n",
        "\n",
        "  X_test = pd.read_csv(at.folder_path / 'X_test.csv')\n",
        "  y_test = pd.read_csv(at.folder_path / 'y_test.csv')\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "OFdr7RAY4x9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.1 Generate Predictions with Baseline Model"
      ],
      "metadata": {
        "id": "4_tQitOfeDXr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mh6epkAThez5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# --- normalize y to Series ---\n",
        "def _norm_y(y):\n",
        "    if y is None: return None\n",
        "    return y.iloc[:,0] if isinstance(y, pd.DataFrame) else pd.Series(y)\n",
        "\n",
        "y_train = _norm_y(globals().get(\"y_train\"))\n",
        "y_val   = _norm_y(globals().get(\"y_val\"))\n",
        "y_test  = _norm_y(globals().get(\"y_test\"))\n",
        "\n",
        "# --- preprocessing (OHE for categoricals, scale numerics) ---\n",
        "num_cols = [c for c in X_train.columns if np.issubdtype(X_train[c].dtype, np.number)]\n",
        "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", make_pipeline(SimpleImputer(strategy=\"median\"),\n",
        "                              StandardScaler(with_mean=False)), num_cols),\n",
        "        (\"cat\", make_pipeline(SimpleImputer(strategy=\"most_frequent\"),\n",
        "                              OneHotEncoder(handle_unknown=\"ignore\")), cat_cols),\n",
        "    ],\n",
        "    remainder=\"drop\", sparse_threshold=1.0\n",
        ")\n",
        "\n",
        "# --- choose model safely ---\n",
        "n_classes = pd.Series(y_train).nunique(dropna=True)\n",
        "\n",
        "if n_classes >= 2:\n",
        "    # Normal path: train a real classifier\n",
        "    clf = make_pipeline(preprocess,\n",
        "                        LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n",
        "    clf.fit(X_train, y_train)\n",
        "    note = \"LogisticRegression\"\n",
        "else:\n",
        "    # Fallback: only one class in training data → use most-frequent baseline\n",
        "    # This prevents: ValueError: This solver needs samples of at least 2 classes...\n",
        "    clf = make_pipeline(preprocess,\n",
        "                        DummyClassifier(strategy=\"most_frequent\", random_state=RANDOM_STATE))\n",
        "    clf.fit(X_train, y_train)  # trains without error\n",
        "    note = \"DummyClassifier (most_frequent) — single-class training set\"\n",
        "\n",
        "# --- predictions ---\n",
        "y_pred_train = clf.predict(X_train)\n",
        "y_pred_val   = clf.predict(X_val)\n",
        "y_pred_test  = clf.predict(X_test)\n",
        "\n",
        "# --- metrics (guarded) ---\n",
        "try:\n",
        "    acc = accuracy_score(y_val, y_pred_val)\n",
        "    f1  = f1_score(y_val, y_pred_val, average=\"macro\", zero_division=0)\n",
        "    print({\"model\": note, \"val_accuracy\": round(acc, 4), \"val_macroF1\": round(f1, 4)})\n",
        "except Exception as e:\n",
        "    print({\"model\": note, \"metric_info\": \"Could not compute classification metrics\", \"error\": str(e)})\n",
        "\n",
        "print(\"Predictions ready → y_pred_train / y_pred_val / y_pred_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.2 Selection of Performance Metrics\n",
        "\n",
        "> Provide some explanations on why you believe the performance metrics you chose is appropriate\n"
      ],
      "metadata": {
        "id": "n5ApYC8BeLsB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7We16YIYhez5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score, confusion_matrix,\n",
        "    mean_absolute_error, mean_squared_error, r2_score\n",
        ")\n",
        "\n",
        "def _is_classification(y: pd.Series) -> bool:\n",
        "    if y is None:\n",
        "        return False\n",
        "    if y.dtype == \"object\" or str(y.dtype) == \"category\":\n",
        "        return True\n",
        "    return pd.Series(y).nunique(dropna=True) <= 20\n",
        "\n",
        "# Ensure y_val / y_pred_val exist and are 1-D\n",
        "assert \"y_val\" in globals() and y_val is not None, \"Need y_val from previous step.\"\n",
        "assert \"y_pred_val\" in globals(), \"Need y_pred_val from baseline step.\"\n",
        "\n",
        "y_true = y_val if isinstance(y_val, pd.Series) else pd.Series(y_val)\n",
        "y_pred = y_pred_val if isinstance(y_pred_val, pd.Series) else pd.Series(y_pred_val)\n",
        "\n",
        "# ---- classification vs regression ----\n",
        "if _is_classification(y_true):\n",
        "    # Metrics chosen:\n",
        "    # - Accuracy: simple baseline understanding\n",
        "    # - Macro-F1: balances precision/recall per class and treats classes equally (good for imbalance)\n",
        "    # - Macro Precision/Recall: show trade-off explicitly\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1m = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    prm = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    rcl = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "\n",
        "    # quick class balance view (helps justify macro-F1)\n",
        "    class_counts = y_true.value_counts(dropna=False).to_dict()\n",
        "\n",
        "    print({\"val_accuracy\": round(acc, 4),\n",
        "           \"val_macroF1\": round(f1m, 4),\n",
        "           \"val_macroPrecision\": round(prm, 4),\n",
        "           \"val_macroRecall\": round(rcl, 4)})\n",
        "    print(\"Class distribution (val):\", class_counts)\n",
        "\n",
        "    # confusion matrix for quick sanity\n",
        "    try:\n",
        "        cm = pd.DataFrame(confusion_matrix(y_true, y_pred),\n",
        "                          index=[f\"true_{c}\" for c in sorted(y_true.unique())],\n",
        "                          columns=[f\"pred_{c}\" for c in sorted(y_true.unique())])\n",
        "        display(cm)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    performance_metrics_explanations = f\"\"\"\n",
        "We report **Accuracy** and **Macro-F1** (with Macro-Precision/Recall) on the validation\n",
        "set. Accuracy provides an intuitive baseline. However, the class distribution is\n",
        "{class_counts}, indicating potential imbalance; therefore **Macro-F1** is preferred as\n",
        "it averages F1 **per class** and treats minority classes equally. Macro-Precision and\n",
        "Macro-Recall expose the precision–recall trade-off, which is important for deciding how\n",
        "to tune thresholds later (e.g., prioritising recall when missing a positive is costly).\n",
        "\"\"\"\n",
        "\n",
        "else:\n",
        "    # Metrics chosen:\n",
        "    # - MAE: easy to interpret in original units\n",
        "    # - RMSE: penalises large errors (sensitive to outliers)\n",
        "    # - R²: goodness of fit (unitless)\n",
        "    mae  = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    r2   = r2_score(y_true, y_pred)\n",
        "\n",
        "    print({\"val_MAE\": round(mae, 4), \"val_RMSE\": round(rmse, 4), \"val_R2\": round(r2, 4)})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "performance_metrics_explanations = \"\"\"\n",
        "**MAE** is directly\n",
        "interpretable in the target's units and is robust to outliers. **RMSE** penalises\n",
        "large errors more heavily, which is useful when big misses are costly. **R²** shows\n",
        "overall goodness-of-fit (unitless) and complements the scale-dependent error metrics.\n",
        "Together these provide a balanced view of typical error size, outlier sensitivity, and fit quality.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BiBsv8S_QyD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='performance_metrics_explanations', value=performance_metrics_explanations)"
      ],
      "metadata": {
        "id": "Ln-xOAkgQyLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.3 Baseline Model Performance\n",
        "\n",
        "> Provide some explanations on model performance\n"
      ],
      "metadata": {
        "id": "q43YtqpdeniY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1Q3oxoNhez5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score, confusion_matrix,\n",
        "    mean_absolute_error, mean_squared_error, r2_score\n",
        ")\n",
        "\n",
        "def _is_classification(y: pd.Series) -> bool:\n",
        "    if y is None:\n",
        "        return False\n",
        "    if y.dtype == \"object\" or str(y.dtype) == \"category\":\n",
        "        return True\n",
        "    return pd.Series(y).nunique(dropna=True) <= 20\n",
        "\n",
        "# Ensure y_val / y_pred_val exist and are 1-D\n",
        "assert \"y_val\" in globals() and y_val is not None, \"Need y_val from previous step.\"\n",
        "assert \"y_pred_val\" in globals(), \"Need y_pred_val from baseline step.\"\n",
        "\n",
        "y_true = y_val if isinstance(y_val, pd.Series) else pd.Series(y_val)\n",
        "y_pred = y_pred_val if isinstance(y_pred_val, pd.Series) else pd.Series(y_pred_val)\n",
        "\n",
        "# classification vs regression\n",
        "if _is_classification(y_true):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1m = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    prm = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    rcl = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "\n",
        "    # quick class balance view (helps justify macro-F1)\n",
        "    class_counts = y_true.value_counts(dropna=False).to_dict()\n",
        "\n",
        "    print({\"val_accuracy\": round(acc, 4),\n",
        "           \"val_macroF1\": round(f1m, 4),\n",
        "           \"val_macroPrecision\": round(prm, 4),\n",
        "           \"val_macroRecall\": round(rcl, 4)})\n",
        "    print(\"Class distribution (val):\", class_counts)\n",
        "\n",
        "    # confusion matrix for quick sanity\n",
        "    try:\n",
        "        cm = pd.DataFrame(confusion_matrix(y_true, y_pred),\n",
        "                          index=[f\"true_{c}\" for c in sorted(y_true.unique())],\n",
        "                          columns=[f\"pred_{c}\" for c in sorted(y_true.unique())])\n",
        "        display(cm)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "else:\n",
        "    # Metrics chosen:\n",
        "    # - MAE: easy to interpret in original units\n",
        "    # - RMSE: penalises large errors (sensitive to outliers)\n",
        "    # - R²: goodness of fit (unitless)\n",
        "    mae  = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    r2   = r2_score(y_true, y_pred)\n",
        "\n",
        "    print({\"val_MAE\": round(mae, 4), \"val_RMSE\": round(rmse, 4), \"val_R2\": round(r2, 4)})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_performance_explanations = \"\"\"\n",
        "The baseline model achieved perfect scores (Accuracy, Macro-F1, Macro Precision, and Macro Recall all equal to 1.0) on the validation set. However, the class distribution on the validation set shows that only a single class (class 0) is present. This indicates that the model, which was a DummyClassifier predicting the most frequent class due to the training data also containing only one class, is simply predicting the majority class for all instances in the validation set. Therefore, these perfect scores do not represent a meaningful evaluation of the model's ability to distinguish between different classes, as there is only one class to predict. This highlights an issue with the dataset split where the validation set does not contain instances of all classes.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "b7S2t4bCQ3Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='baseline_performance_explanations', value=baseline_performance_explanations)"
      ],
      "metadata": {
        "id": "Am6ovHycQ3d8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}