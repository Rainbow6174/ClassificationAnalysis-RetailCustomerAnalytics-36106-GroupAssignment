{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ6wc2HE0pke"
      },
      "source": [
        "# **Baseline Notebook**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFVpE17Ahezu"
      },
      "source": [
        "---\n",
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S8jFaNXqvV5W"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] 拒绝访问。: 'C:\\\\Users\\\\brohao\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\~-0earn\\\\.libs\\\\msvcp140.dll'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n",
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You can now save your data files in: c:\\Users\\brohao\\Desktop\\UTS\\36106\\ClassificationAnalysis-36106-GroupAssignment\\36106\\assignment\\AT3\\data\n"
          ]
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "!pip install -q utstd\n",
        "\n",
        "from utstd.folders import *\n",
        "from utstd.ipyrenders import *\n",
        "\n",
        "at = AtFolder(\n",
        "    course_code=36106,\n",
        "    assignment=\"AT3\",\n",
        ")\n",
        "at.run()\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv9hJ8IO8MTh"
      },
      "source": [
        "---\n",
        "## Student Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ot2igNok8NbA"
      },
      "outputs": [],
      "source": [
        "\n",
        "group_name = \"Group 12\"\n",
        "student_name = \"Jiayu Hao\"\n",
        "student_id = \"25948860\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nMGPhFYunsRN"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">group_name</p><h1 font-size: 3em>Group 12</h1>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h1\", key='group_name', value=group_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NE3R8WY98N_A"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">student_name</p><h1 font-size: 3em>Jiayu Hao</h1>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h1\", key='student_name', value=student_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "anavsHjV8OM5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">student_id</p><h1 font-size: 3em>25948860</h1>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h1\", key='student_id', value=student_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is0ghwXODHkR"
      },
      "source": [
        "---\n",
        "## 0. Python Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgTrMfyylVLf"
      },
      "source": [
        "### 0.a Install Additional Packages\n",
        "\n",
        "> If you are using additional packages, you need to install them here using the command: `! pip install <package_name>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "D79tb2V-lVpJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\brohao\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\brohao\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\brohao\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\brohao\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\brohao\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\brohao\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "! pip install numpy\n",
        "! pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXFKfa2tp1ch"
      },
      "source": [
        "### 0.b Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBEAwdncnlAx"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import altair as alt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8MNBrC4Zgz6"
      },
      "source": [
        "---\n",
        "## A. Assess Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OFdr7RAY4x9x"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "# Load data\n",
        "try:\n",
        "  X_train = pd.read_csv(at.folder_path / 'X_train.csv')\n",
        "  y_train = pd.read_csv(at.folder_path / 'y_train.csv')\n",
        "\n",
        "  X_val = pd.read_csv(at.folder_path / 'X_val.csv')\n",
        "  y_val = pd.read_csv(at.folder_path / 'y_val.csv')\n",
        "\n",
        "  X_test = pd.read_csv(at.folder_path / 'X_test.csv')\n",
        "  y_test = pd.read_csv(at.folder_path / 'y_test.csv')\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_tQitOfeDXr"
      },
      "source": [
        "### A.1 Generate Predictions with Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   customer_key  freq  qty_sum  n_products  revenue_sum   avg_price  \\\n",
            "0         20946     1        5           2     166.9600   41.740000   \n",
            "1         14652     1        8           3     875.1243  175.024860   \n",
            "2         28123     1        3           1       4.9900    4.990000   \n",
            "3         13498     1        4           3     100.0900   33.363333   \n",
            "4         28231     1        9           2      88.2500   17.650000   \n",
            "\n",
            "   avg_margin  returns_total  age  annual_income  number_dependents  \\\n",
            "0    0.328998          371.0   46        30000.0                0.0   \n",
            "1    0.597124          191.0   62        60000.0                1.0   \n",
            "2    0.625992           69.0   47        40000.0                0.0   \n",
            "3    0.625997          160.0   60        30000.0                4.0   \n",
            "4    0.625983          443.0   66        80000.0                2.0   \n",
            "\n",
            "       education_level      occupation marital_status homeowner  orders_2020  \\\n",
            "0      Partial College        Clerical              S         N          0.0   \n",
            "1      Partial College  Skilled Manual              M         Y          0.0   \n",
            "2          High School  Skilled Manual              S         Y          0.0   \n",
            "3      Graduate Degree        Clerical              S         Y          0.0   \n",
            "4  Partial High School  Skilled Manual              S         Y          0.0   \n",
            "\n",
            "   orders_2021  orders_trend  \n",
            "0          0.0           0.0  \n",
            "1          0.0           0.0  \n",
            "2          0.0           0.0  \n",
            "3          0.0           0.0  \n",
            "4          0.0           0.0  \n",
            "purchase_next_3m    0.111173\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# dataset overview\n",
        "print(X_train.head(5))\n",
        "print(y_train.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Mh6epkAThez5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drop columns: ['customer_key']\n"
          ]
        }
      ],
      "source": [
        "# drop id-like column\n",
        "id_like = ['customer_key']\n",
        "\n",
        "drop_cols = list(set(id_like))\n",
        "print(\"Drop columns:\", drop_cols)\n",
        "\n",
        "X_train = X_train.drop(columns=drop_cols, errors='ignore')\n",
        "X_val   = X_val.drop(columns=drop_cols, errors='ignore')\n",
        "X_test  = X_test.drop(columns=drop_cols, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# numerical column and categorical column\n",
        "num_cols = X_train.select_dtypes(include=['number','float','int','Int64']).columns.tolist()\n",
        "cat_cols = X_train.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
        "\n",
        "# sparse preparation (For LR) \n",
        "pre_sparse = ColumnTransformer([\n",
        "    ('num', StandardScaler(with_mean=False), num_cols),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), cat_cols)\n",
        "], remainder='drop')\n",
        "\n",
        "baseline_lr = Pipeline([\n",
        "    ('prep', pre_sparse),\n",
        "    ('clf', LogisticRegression(max_iter=2000, class_weight='balanced', solver='liblinear'))\n",
        "])\n",
        "\n",
        "# Fitting LR baseline\n",
        "baseline_lr.fit(X_train, y_train)\n",
        "\n",
        "p_val  = baseline_lr.predict_proba(X_val)[:,1]\n",
        "p_test = baseline_lr.predict_proba(X_test)[:,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5ApYC8BeLsB"
      },
      "source": [
        "### A.2 Selection of Performance Metrics\n",
        "\n",
        "> Provide some explanations on why you believe the performance metrics you chose is appropriate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7We16YIYhez5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL  ROC-AUC/PR-AUC: 0.664 0.268\n",
            "TEST ROC-AUC/PR-AUC: 0.649 0.234\n"
          ]
        }
      ],
      "source": [
        "print(\"VAL  ROC-AUC/PR-AUC:\", round(roc_auc_score(y_val, p_val),3), round(average_precision_score(y_val, p_val),3))\n",
        "print(\"TEST ROC-AUC/PR-AUC:\", round(roc_auc_score(y_test, p_test),3), round(average_precision_score(y_test, p_test),3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BiBsv8S_QyD-"
      },
      "outputs": [],
      "source": [
        "# Provide some explanations on why you believe the performance metrics you chose is appropriate\n",
        "\n",
        "performance_metrics_explanations = \"\"\"\n",
        "The selected metrics, ROC-AUC and PR-AUC, are appropriate because the \n",
        "dataset is imbalanced (only about 11% positive class), and these metrics \n",
        "evaluate model discrimination more reliably than accuracy. ROC-AUC measures \n",
        "overall separability between buyers and non-buyers, independent of any \n",
        "threshold, while PR-AUC focuses on the model’s ability to identify true \n",
        "buyers among the top-ranked predictions, directly reflecting marketing \n",
        "efficiency. Using these metrics allows us to fairly assess performance \n",
        "across all probability levels and later convert the ranking quality into \n",
        "business value through cost–benefit threshold tuning.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ln-xOAkgQyLh"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">performance_metrics_explanations</p><h3 font-size: 3em>\n",
              "The selected metrics, ROC-AUC and PR-AUC, are appropriate because the \n",
              "dataset is imbalanced (only about 11% positive class), and these metrics \n",
              "evaluate model discrimination more reliably than accuracy. ROC-AUC measures \n",
              "overall separability between buyers and non-buyers, independent of any \n",
              "threshold, while PR-AUC focuses on the model’s ability to identify true \n",
              "buyers among the top-ranked predictions, directly reflecting marketing \n",
              "efficiency. Using these metrics allows us to fairly assess performance \n",
              "across all probability levels and later convert the ranking quality into \n",
              "business value through cost–benefit threshold tuning.\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='performance_metrics_explanations', value=performance_metrics_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q43YtqpdeniY"
      },
      "source": [
        "### A.3 Baseline Model Performance\n",
        "\n",
        "> Provide some explanations on model performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "x1Q3oxoNhez5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best threshold on VAL: 0.01 | Expected gain: -3932100\n",
            "\n",
            "TEST (0.50) @thr=0.50\n",
            "[[444 241]\n",
            " [ 40  46]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.917     0.648     0.760       685\n",
            "           1      0.160     0.535     0.247        86\n",
            "\n",
            "    accuracy                          0.636       771\n",
            "   macro avg      0.539     0.592     0.503       771\n",
            "weighted avg      0.833     0.636     0.702       771\n",
            "\n",
            "\n",
            "TEST (best_thr) @thr=0.01\n",
            "[[  0 685]\n",
            " [  0  86]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.000     0.000     0.000       685\n",
            "           1      0.112     1.000     0.201        86\n",
            "\n",
            "    accuracy                          0.112       771\n",
            "   macro avg      0.056     0.500     0.100       771\n",
            "weighted avg      0.012     0.112     0.022       771\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Simple threshold ( = 0.5) compare with optimal threshold\n",
        "\n",
        "# def standard of gain function\n",
        "W_TP, W_FP, W_FN, W_TN = 100, -20, -80, 0  \n",
        "# choose threshold\n",
        "def pick_best_threshold(y_true, p):\n",
        "    thrs = np.linspace(0.01,0.99,99)\n",
        "    best_t, best_gain = 0.5, -1e18\n",
        "    for t in thrs:\n",
        "        pred = (p>=t).astype(int)\n",
        "        TP = ((pred==1)&(y_true==1)).sum()\n",
        "        FP = ((pred==1)&(y_true==0)).sum()\n",
        "        FN = ((pred==0)&(y_true==1)).sum()\n",
        "        TN = ((pred==0)&(y_true==0)).sum()\n",
        "        gain = TP*W_TP + FP*W_FP + FN*W_FN + TN*W_TN\n",
        "        if gain > best_gain:\n",
        "            best_gain, best_t = gain, t\n",
        "    return best_t, best_gain\n",
        "\n",
        "best_thr, best_gain = pick_best_threshold(y_val.values, p_val)\n",
        "print(f\"Best threshold on VAL: {best_thr:.2f} | Expected gain: {best_gain:.0f}\")\n",
        "# plot performance\n",
        "def show_eval(tag, y, p, thr):\n",
        "    pred = (p>=thr).astype(int)\n",
        "    print(f\"\\n{tag} @thr={thr:.2f}\")\n",
        "    print(confusion_matrix(y, pred))\n",
        "    print(classification_report(y, pred, digits=3))\n",
        "\n",
        "show_eval(\"TEST (0.50)\", y_test.values, p_test, 0.50)\n",
        "show_eval(\"TEST (best_thr)\", y_test.values, p_test, best_thr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b7S2t4bCQ3Y3"
      },
      "outputs": [],
      "source": [
        "# Provide some explanations on model performance\n",
        "baseline_performance_explanations = \"\"\"\n",
        "The baseline logistic regression shows moderate performance (ROC-AUC = 0.65, \n",
        "PR-AUC = 0.23), meaning it can separate buyers from non-buyers better than \n",
        "random but still misses many true positives. At the default threshold (0.5), \n",
        "it achieves good precision but low recall; \n",
        "the optimal threshold (0.01) gives unrealistic all-positive predictions \n",
        "because the model’s probabilities are poorly calibrated. The large negative \n",
        "expected gain reflects this imbalance and the unscaled cost weights. \n",
        "Overall, the model is a reasonable interpretative baseline but needs \n",
        "nonlinear and calibrated methods to improve decision quality.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Am6ovHycQ3d8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p style=\"color:grey\">baseline_performance_explanations</p><h3 font-size: 3em>\n",
              "The baseline logistic regression shows moderate performance (ROC-AUC = 0.65, \n",
              "PR-AUC = 0.23), meaning it can separate buyers from non-buyers better than \n",
              "random but still misses many true positives. At the default threshold (0.5), \n",
              "it achieves good precision but low recall; \n",
              "the optimal threshold (0.01) gives unrealistic all-positive predictions \n",
              "because the model’s probabilities are poorly calibrated. The large negative \n",
              "expected gain reflects this imbalance and the unscaled cost weights. \n",
              "Overall, the model is a reasonable interpretative baseline but needs \n",
              "nonlinear and calibrated methods to improve decision quality.\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='baseline_performance_explanations', value=baseline_performance_explanations)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
