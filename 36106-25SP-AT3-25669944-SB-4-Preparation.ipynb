{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ6wc2HE0pke"
      },
      "source": [
        "# **Preparation Notebook**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFVpE17Ahezu"
      },
      "source": [
        "---\n",
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "!pip install -q utstd\n",
        "\n",
        "from utstd.folders import *\n",
        "from utstd.ipyrenders import *\n",
        "\n",
        "at = AtFolder(\n",
        "    course_code=36106,\n",
        "    assignment=\"AT3\",\n",
        ")\n",
        "at.run()\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')"
      ],
      "metadata": {
        "id": "S8jFaNXqvV5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Student Information"
      ],
      "metadata": {
        "id": "ujC3TjlB7-EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <Student to fill this section and then remove this comment>\n",
        "group_name = \"Group 12\"\n",
        "student_name = \"Victor Rono\"\n",
        "student_id = \"25669944\""
      ],
      "metadata": {
        "id": "dug0CHu27_wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h1\", key='group_name', value=group_name)"
      ],
      "metadata": {
        "id": "2jXi_XvEuChh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h1\", key='student_name', value=student_name)"
      ],
      "metadata": {
        "id": "SeSMrvjD8AIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h1\", key='student_id', value=student_id)"
      ],
      "metadata": {
        "id": "Utiw0TD68JjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 0. Python Packages"
      ],
      "metadata": {
        "id": "sO3Cb4F0DrGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.a Install Additional Packages\n",
        "\n",
        "> If you are using additional packages, you need to install them here using the command: `! pip install <package_name>`"
      ],
      "metadata": {
        "id": "CgTrMfyylVLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q utstd scikit-learn==1.5.0 numpy==1.26.0 scipy==1.11.0"
      ],
      "metadata": {
        "id": "D79tb2V-lVpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.b Import Packages"
      ],
      "metadata": {
        "id": "mXFKfa2tp1ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "import pandas as pd\n",
        "import altair as alt"
      ],
      "metadata": {
        "id": "GBEAwdncnlAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NCwQQFkU3v5"
      },
      "source": [
        "---\n",
        "## A. Feature Selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.0 Load Data"
      ],
      "metadata": {
        "id": "PXegqMWYt6TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "# Load datasets\n",
        "try:\n",
        "  sales_2022_df = pd.read_csv(at.folder_path / \"sales_2022.csv\")\n",
        "  products_df = pd.read_csv(at.folder_path / \"products.csv\")\n",
        "  product_subcats_df = pd.read_csv(at.folder_path / \"product_subcats.csv\")\n",
        "  territories_df = pd.read_csv(at.folder_path / \"territories.csv\")\n",
        "  sales_2021_df = pd.read_csv(at.folder_path / \"sales_2021.csv\")\n",
        "  returns_df = pd.read_csv(at.folder_path / \"returns.csv\")\n",
        "  sales_2020_df = pd.read_csv(at.folder_path / \"sales_2020.csv\")\n",
        "  product_cats_df = pd.read_csv(at.folder_path / \"product_cats.csv\")\n",
        "  customers_df = pd.read_csv(at.folder_path / \"customers.csv\")\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "EGLh1LG_Hvz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.1 Approach 1"
      ],
      "metadata": {
        "id": "E18dcL6C3O-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiroVmhn9_HP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "prep = customers_df.copy()  # work on a copy\n",
        "\n",
        "# 1. Age from birth_date if age missing (fixed reference date for reproducibility)\n",
        "if \"age\" not in prep.columns and \"birth_date\" in prep.columns:\n",
        "    prep[\"birth_date\"] = pd.to_datetime(prep[\"birth_date\"], errors=\"coerce\")\n",
        "    prep[\"age\"] = ((pd.Timestamp(\"2022-01-01\") - prep[\"birth_date\"]).dt.days/365.25).round(0)\n",
        "\n",
        "# 2. Age band (lifestage) — easy to explain to stakeholders\n",
        "if \"age\" in prep.columns and \"age_group\" not in prep.columns:\n",
        "    prep[\"age_group\"] = pd.cut(prep[\"age\"], bins=[0,25,45,65,150],\n",
        "                               labels=[\"Youth\",\"Adult\",\"Middle Age\",\"Senior\"])\n",
        "\n",
        "# 3. Income band — quick purchasing-power proxy\n",
        "if \"annual_income\" in prep.columns and \"income_category\" not in prep.columns:\n",
        "    prep[\"income_category\"] = pd.cut(prep[\"annual_income\"], bins=[0,30000,60000,100000,1e12],\n",
        "                                     labels=[\"Low\",\"Middle\",\"Upper Middle\",\"High\"])\n",
        "\n",
        "# 4. Family status — simple dependent flag\n",
        "if \"family_status\" not in prep.columns:\n",
        "    deps = prep.get(\"number_dependents\", np.nan)\n",
        "    prep[\"family_status\"] = np.where(pd.to_numeric(deps, errors=\"coerce\").fillna(0)>0,\n",
        "                                     \"Has Dependents\",\"No Dependents\")\n",
        "\n",
        "# 5. Value segment — income quartiles (robust to skew)\n",
        "if \"value_segment\" not in prep.columns and \"annual_income\" in prep.columns:\n",
        "    try:\n",
        "        prep[\"value_segment\"] = pd.qcut(prep[\"annual_income\"], q=4,\n",
        "                                        labels=[\"Low\",\"Mid-Low\",\"Mid-High\",\"High\"])\n",
        "    except Exception:\n",
        "        # if many ties/NA prevent qcut, fall back to simple bins\n",
        "        prep[\"value_segment\"] = pd.cut(prep[\"annual_income\"], bins=[-np.inf,0,60000,100000,np.inf],\n",
        "                                       labels=[\"NA/Zero\",\"≤60k\",\"60-100k\",\">100k\"])\n",
        "\n",
        "print(\"Engineered columns added:\",\n",
        "      [c for c in [\"age_group\",\"income_category\",\"family_status\",\"value_segment\"] if c in prep.columns])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_selection_1_insights = \"\"\"\n",
        "**Approach 1 — Business-driven, interpretable features**\n",
        "\n",
        "• Focus on lifestage and purchasing power signals that stakeholders understand:\n",
        "  age / age_group, annual_income / income_category, family_status, homeowner, education, occupation.\n",
        "• Light engineering only (bands + flags) to keep transparency and reduce leakage risk.\n",
        "• Result: {X_a1.shape[1]} features selected (columns: {', '.join(X_a1.columns[:6])}{'…' if X_a1.shape[1]>6 else ''}).\n",
        "• Ready for simple encoding & scaling; redundant score fields will be removed in Approach 2.\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "ynIRSpW6KoH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='feature_selection_1_insights', value=feature_selection_1_insights)"
      ],
      "metadata": {
        "id": "7JRXCIVFPHlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.2 Approach 2"
      ],
      "metadata": {
        "id": "m0WdHbsf3vTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. start from Approach-1 output if available; otherwise from the loaded customers table\n",
        "base = None\n",
        "if isinstance(globals().get(\"X_a1\"), pd.DataFrame) and not globals().get(\"X_a1\").empty:\n",
        "    base = globals().get(\"X_a1\")\n",
        "elif isinstance(globals().get(\"prep\"), pd.DataFrame) and not globals().get(\"prep\").empty:\n",
        "    base = globals().get(\"prep\")\n",
        "elif isinstance(globals().get(\"customers_df\"), pd.DataFrame) and not globals().get(\"customers_df\").empty:\n",
        "    base = globals().get(\"customers_df\")\n",
        "\n",
        "assert base is not None and not base.empty, \"Run A.1 first (or ensure 'customers' is loaded).\"\n",
        "\n",
        "X2 = base.copy()\n",
        "\n",
        "# 2. drop obvious redundant “score” fields (e.g., satisfaction_score, risk_score)\n",
        "dropped_scores = [c for c in X2.columns if c.endswith(\"_score\")]\n",
        "X2 = X2.drop(columns=dropped_scores, errors=\"ignore\")\n",
        "\n",
        "# 3. remove one of any pair of highly-correlated numeric columns (|r| > 0.95)\n",
        "num_cols = X2.select_dtypes(include=\"number\").columns.tolist()\n",
        "dropped_corr = []\n",
        "if len(num_cols) >= 2:\n",
        "    corr = X2[num_cols].corr().abs()\n",
        "    # keep a simple upper-triangle scan\n",
        "    keep = set()\n",
        "    for i, c1 in enumerate(num_cols):\n",
        "        if c1 in dropped_corr:\n",
        "            continue\n",
        "        keep.add(c1)\n",
        "        for c2 in num_cols[i+1:]:\n",
        "            if corr.loc[c1, c2] > 0.95:\n",
        "                dropped_corr.append(c2)\n",
        "    X2 = X2.drop(columns=dropped_corr, errors=\"ignore\")\n",
        "\n",
        "# 4. output for downstream steps\n",
        "X_a2 = X2.copy()             # Approach-2 dataset\n",
        "features_2 = X_a2.columns.tolist()\n",
        "print({\"kept_features\": len(features_2),\n",
        "       \"dropped_scores\": dropped_scores,\n",
        "       \"dropped_high_corr\": dropped_corr})\n",
        "X_a2.head(3)"
      ],
      "metadata": {
        "id": "2aAKCI6v3xbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_selection_2_insights = \"\"\"\n",
        "**Approach 2 — De-redundancy & stability**\n",
        "\n",
        "• Removed superficial *_score fields to avoid double-counting and improve interpretability.\n",
        "• Pruned highly-correlated numeric pairs (|r|>0.95) to reduce multicollinearity risk.\n",
        "• Result: kept **{len(features_2)}** features; dropped scores: {len(globals().get('dropped_scores', []))},\n",
        "  high-corr removed: {len(globals().get('dropped_corr', []))}.\n",
        "• This yields a lean, stable set that encodes/scales cleanly and supports fair model comparison.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZETRSFpiPlYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='feature_selection_2_insights', value=feature_selection_2_insights)"
      ],
      "metadata": {
        "id": "TYTclH9HPlda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.n Approach \"\\<describe_approach_here\\>\"\n",
        "\n",
        "> You can add more cells related to other approaches in this section"
      ],
      "metadata": {
        "id": "x0E3hYLwK-PQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "base = None\n",
        "if isinstance(globals().get(\"X_a2\"), pd.DataFrame) and not globals().get(\"X_a2\").empty:\n",
        "    base = globals().get(\"X_a2\")\n",
        "elif isinstance(globals().get(\"X_a1\"), pd.DataFrame) and not globals().get(\"X_a1\").empty:\n",
        "    base = globals().get(\"X_a1\")\n",
        "elif isinstance(globals().get(\"customers_df\"), pd.DataFrame) and not globals().get(\"customers_df\").empty:\n",
        "    base = globals().get(\"customers_df\")\n",
        "\n",
        "assert base is not None and not base.empty, \"Run A.1/A.2 first (or ensure 'customers' is loaded).\"\n",
        "\n",
        "Xn = base.copy()\n",
        "\n",
        "# 2) Split numeric vs categorical once\n",
        "num_cols = [c for c in Xn.columns if pd.api.types.is_numeric_dtype(Xn[c])]\n",
        "cat_cols = [c for c in Xn.columns if c not in num_cols]\n",
        "\n",
        "# 3) Drop near-constant numeric features (very low variance → little signal)\n",
        "near_const = []\n",
        "for c in num_cols:\n",
        "    v = np.nanvar(pd.to_numeric(Xn[c], errors=\"coerce\"))\n",
        "    if v < 1e-6:                 # threshold kept simple and transparent\n",
        "        near_const.append(c)\n",
        "Xn = Xn.drop(columns=near_const, errors=\"ignore\")\n",
        "\n",
        "# 4) Rare-category grouping for high-cardinality categoricals\n",
        "#  Any level with <1% frequency becomes 'Other' (keeps models stable and interpretable)\n",
        "rarity_threshold = 0.01\n",
        "grouped_info = {}\n",
        "for c in cat_cols:\n",
        "    if c not in Xn.columns:\n",
        "        continue\n",
        "    # --- MODIFICATION: Skip rarity grouping for 'customer_id' ---\n",
        "    if c in [\"customer_id\", \"CustomerID\", \"CustomerKey\", \"customer_key\", \"id\", \"ID\"]:\n",
        "        continue\n",
        "    # --- End Modification ---\n",
        "\n",
        "    freq = Xn[c].astype(str).value_counts(normalize=True, dropna=False)\n",
        "    rare_levels = freq[freq < rarity_threshold].index.tolist()\n",
        "    if rare_levels:\n",
        "        grouped_info[c] = len(rare_levels)\n",
        "        Xn[c] = Xn[c].astype(str).where(~Xn[c].astype(str).isin(rare_levels), \"Other\")\n",
        "\n",
        "# 5) Output for downstream steps\n",
        "X_an = Xn.copy()   # Approach-n dataset\n",
        "kept_cols_n = X_an.columns.tolist()\n",
        "print({\n",
        "    \"kept_features\": len(kept_cols_n),\n",
        "    \"near_constant_dropped\": near_const,\n",
        "    \"cats_grouped\": grouped_info\n",
        "})\n",
        "display(X_an.head(3))"
      ],
      "metadata": {
        "id": "S54DYl_fK7_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DwaOOhlz_mnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_selection_n_insights = \"\"\"\n",
        "**Approach n — Unsupervised filters (variance + rarity)**\n",
        "• Removed near-constant numeric features to avoid noise: {len(globals().get('near_const', []))} dropped.\n",
        "• Grouped rare categorical levels (<1% frequency) to 'Other' for stability: {sum(globals().get('grouped_info', {}).values())} levels grouped across {len(globals().get('grouped_info', {}))} fields.\n",
        "• Result: **{len(globals().get('kept_cols_n', X_an.columns).tolist() if isinstance(X_an, pd.DataFrame) else X_an.shape[1])}** features retained; cleaner, more robust inputs for encoding/scaling.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xJ6-jVSE_mxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='feature_selection_n_insights', value=feature_selection_n_insights)"
      ],
      "metadata": {
        "id": "luP391ln_nGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.z Final Selection of Features"
      ],
      "metadata": {
        "id": "j5bbQlUn3635"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1) Pick the best prepared table available (prefer the latest approach)\n",
        "base = None\n",
        "if isinstance(globals().get(\"X_an\"), pd.DataFrame) and not globals().get(\"X_an\").empty:\n",
        "    base = globals().get(\"X_an\")\n",
        "elif isinstance(globals().get(\"X_a2\"), pd.DataFrame) and not globals().get(\"X_a2\").empty:\n",
        "    base = globals().get(\"X_a2\")\n",
        "elif isinstance(globals().get(\"X_a1\"), pd.DataFrame) and not globals().get(\"X_a1\").empty:\n",
        "    base = globals().get(\"X_a1\")\n",
        "\n",
        "assert base is not None and not base.empty, \"Run A.1/A.2 (and A.n if used) before this cell.\"\n",
        "\n",
        "# 2) Final feature list = columns of the chosen table\n",
        "features_list = base.columns.tolist()\n",
        "\n",
        "# 3) (Optional) quick sanity peek\n",
        "print({\"final_feature_count\": len(features_list)})\n",
        "print(\"features_list:\", features_list)\n",
        "display(base.head(3))"
      ],
      "metadata": {
        "id": "zfC-DLKv4AuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VeRdhcf-K5H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_selection_explanations = \"\"\"\n",
        "**Final selection — {len(features_list)} features**\n",
        "• Started with a business-interpretable shortlist (A.1),\n",
        "  removed redundant *_score and highly correlated numerics (A.2),\n",
        "  and optionally stabilised with variance/rarity filtering (A.n).\n",
        "• The final set balances interpretability (age/education/occupation/income bands, family/homeowner)\n",
        "  with robustness (reduced multicollinearity; grouped rare levels).\n",
        "• Ready for one-hot encoding + scaling in the next stage (Baseline/Experiment).\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wRkiZf4sPsxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='feature_selection_explanations', value=feature_selection_explanations)"
      ],
      "metadata": {
        "id": "EcG8s40PPs3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## B. Data Cleaning"
      ],
      "metadata": {
        "id": "ErH_bh604mOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.1 Fixing \"\\<describe_issue_here\\>\""
      ],
      "metadata": {
        "id": "bII_PglX5E-r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaR8p5n8hez4"
      },
      "outputs": [],
      "source": [
        "# Inconsistent categorical labels (case / leading-trailing spaces)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Start from the latest prepared table if available; fall back gracefully\n",
        "base = None\n",
        "if isinstance(globals().get(\"X_an\"), pd.DataFrame) and not globals().get(\"X_an\").empty:\n",
        "    base = globals().get(\"X_an\")\n",
        "elif isinstance(globals().get(\"X_a2\"), pd.DataFrame) and not globals().get(\"X_a2\").empty:\n",
        "    base = globals().get(\"X_a2\")\n",
        "elif isinstance(globals().get(\"X_a1\"), pd.DataFrame) and not globals().get(\"X_a1\").empty:\n",
        "    base = globals().get(\"X_a1\")\n",
        "elif isinstance(globals().get(\"customers_df\"), pd.DataFrame) and not globals().get(\"customers_df\").empty:\n",
        "    base = globals().get(\"customers_df\")\n",
        "\n",
        "\n",
        "assert base is not None and not base.empty, \"Run A.1/A.2 (and A.n if used) before B.1.\"\n",
        "\n",
        "X_clean1 = base.copy()\n",
        "\n",
        "# 2) Identify categorical/text columns\n",
        "cat_cols = [c for c in X_clean1.columns\n",
        "            if X_clean1[c].dtype == \"object\" or str(X_clean1[c].dtype) == \"category\"]\n",
        "\n",
        "# 3) Helper: normalise strings → strip spaces, collapse internal whitespace, Title case\n",
        "def _norm_str(s: pd.Series) -> pd.Series:\n",
        "    s = s.astype(str)\n",
        "    s = s.str.replace(r\"\\s+\", \" \", regex=True)  # collapse multiple spaces\n",
        "    s = s.str.strip().replace({\"\": np.nan})     # trim + empty→NaN\n",
        "    return s.str.title()                        # consistent casing\n",
        "\n",
        "# 4) Apply normalisation + simple mode imputation where needed\n",
        "for c in cat_cols:\n",
        "    before = X_clean1[c].value_counts(dropna=False).head(3).to_dict()  # snapshot of top labels\n",
        "    X_clean1[c] = _norm_str(X_clean1[c])\n",
        "    if X_clean1[c].isna().any():\n",
        "        X_clean1[c] = X_clean1[c].fillna(X_clean1[c].mode().iloc[0])\n",
        "    after = X_clean1[c].value_counts(dropna=False).head(3).to_dict()\n",
        "    print(f\"[{c}] top before→after:\", before, \"→\", after)\n",
        "\n",
        "print({\"rows\": len(X_clean1), \"cols\": X_clean1.shape[1], \"cleaned_cat_cols\": len(cat_cols)})"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MP7lpNL0LZBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_cleaning_1_explanations = \"\"\"\n",
        "**Issue fixed:** Inconsistent categorical labels (case, extra spaces, blank strings).\n",
        "\n",
        "**Why it matters**\n",
        "• Duplicate categories like \" single \", \"Single\", and \"SINGLE\" fragment counts and distort one-hot encodings.\n",
        "• Blank strings behave like valid values, leaking noise into models and reports.\n",
        "\n",
        "**What I did**\n",
        "• Normalised all text features: strip spaces, collapse internal whitespace, Title-case.\n",
        "• Converted empty strings to missing values, then **mode-imputed** per column.\n",
        "\n",
        "**Impact**\n",
        "• Fewer spurious levels, stable one-hot dimension, cleaner summary stats; improves downstream model reliability.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dNo--8IgPzhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='data_cleaning_1_explanations', value=data_cleaning_1_explanations)"
      ],
      "metadata": {
        "id": "cz_yVvA-Pzkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.2 Fixing \"\\<describe_issue_here\\>\""
      ],
      "metadata": {
        "id": "wRJ6Ql1F5ODe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsTzKV3bhez4"
      },
      "outputs": [],
      "source": [
        "# Unrealistic numeric outliers (IQR capping with safe fallbacks)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Start from latest cleaned table; fallback gracefully\n",
        "base = None\n",
        "if isinstance(globals().get(\"X_clean1\"), pd.DataFrame) and not globals().get(\"X_clean1\").empty:\n",
        "    base = globals().get(\"X_clean1\")   # from B.1 (categorical normalisation)\n",
        "elif isinstance(globals().get(\"X_an\"), pd.DataFrame) and not globals().get(\"X_an\").empty:\n",
        "    base = globals().get(\"X_an\")    # from A.n\n",
        "elif isinstance(globals().get(\"X_a2\"), pd.DataFrame) and not globals().get(\"X_a2\").empty:\n",
        "    base = globals().get(\"X_a2\")    # from A.2\n",
        "elif isinstance(globals().get(\"X_a1\"), pd.DataFrame) and not globals().get(\"X_a1\").empty:\n",
        "    base = globals().get(\"X_a1\")   # from A.1\n",
        "\n",
        "assert base is not None and not base.empty, \"Run earlier prep steps before B.2.\"\n",
        "\n",
        "X_clean2 = base.copy()\n",
        "\n",
        "# 2) Find numeric columns only\n",
        "num_cols = [c for c in X_clean2.columns if pd.api.types.is_numeric_dtype(X_clean2[c])]\n",
        "\n",
        "capped_summary = {}  # per-column counts of capped values\n",
        "for c in num_cols:\n",
        "    s = pd.to_numeric(X_clean2[c], errors=\"coerce\")\n",
        "\n",
        "    # IQR fences\n",
        "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    if pd.isna(iqr) or iqr == 0:\n",
        "        # Fallback for flat/constant columns: use 1st–99th percentile bounds\n",
        "        lo, hi = s.quantile(0.01), s.quantile(0.99)\n",
        "    else:\n",
        "        lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
        "\n",
        "    # Clip and count how many changed\n",
        "    s_clip = s.clip(lower=lo, upper=hi)\n",
        "    capped_summary[c] = int((s_clip != s).sum())\n",
        "\n",
        "    # Write back (preserve dtype as float to avoid overflow)\n",
        "    X_clean2[c] = s_clip.astype(float)\n",
        "\n",
        "print(\"Capped outliers per column (non-zero only):\")\n",
        "print({k:v for k,v in capped_summary.items() if v})\n",
        "print({\"rows\": len(X_clean2), \"cols\": X_clean2.shape[1]})"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LZwEVfkqLcwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_cleaning_2_explanations = \"\"\"\n",
        "**Issue fixed:** Unrealistic numeric outliers.\n",
        "\n",
        "**Why it matters**\n",
        "• Extreme values skew means/variances, dominate scaling, and can destabilise distance-based models\n",
        "  and regularised linear methods.\n",
        "\n",
        "**What I did**\n",
        "• Applied per-feature robust caps using **IQR fences** (Q1−1.5·IQR, Q3+1.5·IQR);\n",
        "  for flat/zero-IQR columns fell back to **1st–99th percentile** bounds.\n",
        "• Only values beyond bounds were clipped; no rows were dropped.\n",
        "\n",
        "**Impact**\n",
        "• Columns adjusted: {len(changed_cols)} ({', '.join(changed_cols[:6])}{'…' if len(changed_cols)>6 else ''}).\n",
        "• Produces stable scaling, fairer model coefficients, and reduces sensitivity to data entry errors.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zbp9FGOPP7UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='data_cleaning_2_explanations', value=data_cleaning_2_explanations)"
      ],
      "metadata": {
        "id": "51A_eWA0P7W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.3 Fixing \"\\<describe_issue_here\\>\""
      ],
      "metadata": {
        "id": "qhu0LRMi5PgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# duplicate records / repeated IDs (keep one per customer)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Start from the most-recent cleaned table; fall back gracefully\n",
        "base = None\n",
        "if isinstance(globals().get(\"X_clean2\"), pd.DataFrame) and not globals().get(\"X_clean2\").empty:\n",
        "    base = globals().get(\"X_clean2\")  # after B.2 outlier capping\n",
        "elif isinstance(globals().get(\"X_clean1\"), pd.DataFrame) and not globals().get(\"X_clean1\").empty:\n",
        "    base = globals().get(\"X_clean1\")\n",
        "elif isinstance(globals().get(\"X_an\"), pd.DataFrame) and not globals().get(\"X_an\").empty:\n",
        "    base = globals().get(\"X_an\")\n",
        "elif isinstance(globals().get(\"X_a2\"), pd.DataFrame) and not globals().get(\"X_a2\").empty:\n",
        "    base = globals().get(\"X_a2\")\n",
        "elif isinstance(globals().get(\"X_a1\"), pd.DataFrame) and not globals().get(\"X_a1\").empty:\n",
        "    base = globals().get(\"X_a1\")\n",
        "elif isinstance(globals().get(\"customers_df\"), pd.DataFrame) and not globals().get(\"customers_df\").empty:\n",
        "    base = globals().get(\"customers_df\")\n",
        "\n",
        "\n",
        "assert base is not None and not base.empty, \"Run earlier prep steps before B.3.\"\n",
        "\n",
        "df = base.copy()\n",
        "rows_before = len(df)\n",
        "\n",
        "# 2) First remove exact duplicate rows (all columns identical)\n",
        "exact_dups = int(df.duplicated().sum())\n",
        "df = df.drop_duplicates(ignore_index=True)\n",
        "\n",
        "# 3) Identify a customer ID column (robust to different names)\n",
        "id_candidates = [\"CustomerKey\",\"customer_key\",\"CustomerID\",\"customer_id\",\"ID\",\"id\"]\n",
        "id_col = next((c for c in id_candidates if c in df.columns), None)\n",
        "\n",
        "# Optional \"recency\" column to decide which duplicate to keep\n",
        "date_candidates = [\"UpdatedAt\",\"updated_at\",\"LastUpdate\",\"last_update\",\"OrderDate\",\"order_date\"]\n",
        "date_col = next((c for c in date_candidates if c in df.columns), None)\n",
        "if date_col is not None:\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
        "\n",
        "# 4) If we have an ID, keep ONE row per ID (latest by date if available, else first)\n",
        "id_dups = 0\n",
        "if id_col is not None:\n",
        "    if date_col is not None:\n",
        "        # sort so newest comes first; drop duplicates keeping first (newest)\n",
        "        df = df.sort_values(by=date_col, ascending=False)\n",
        "        id_dups = int(df.duplicated(subset=[id_col]).sum())\n",
        "        df = df.drop_duplicates(subset=[id_col], keep=\"first\").sort_index(ignore_index=True)\n",
        "    else:\n",
        "        id_dups = int(df.duplicated(subset=[id_col]).sum())\n",
        "        df = df.drop_duplicates(subset=[id_col], keep=\"first\").reset_index(drop=True)\n",
        "\n",
        "rows_after = len(df)\n",
        "\n",
        "# 5) Output for downstream steps\n",
        "X_clean3 = df.copy()\n",
        "print({\n",
        "    \"rows_before\": rows_before,\n",
        "    \"rows_after\": rows_after,\n",
        "    \"exact_row_duplicates_removed\": exact_dups,\n",
        "    \"id_duplicates_removed\": id_dups,\n",
        "    \"id_col_used\": id_col,\n",
        "    \"recency_col_used\": date_col\n",
        "})\n",
        "display(X_clean3.head(3))"
      ],
      "metadata": {
        "id": "eJh1KWoF5Qze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMKB2aKLLhEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_cleaning_3_explanations = \"\"\"\n",
        "**Issue fixed:** Duplicate records / repeated IDs.\n",
        "\n",
        "**Why it matters**\n",
        "• Duplicates inflate counts, bias aggregates (e.g., spend per customer),\n",
        "  and can leak target information when the same entity appears multiple times.\n",
        "\n",
        "**What I did**\n",
        "• Removed exact duplicate rows across all columns.\n",
        "• Then de-duplicated by **{globals().get('id_col','<ID not found>')}**\n",
        "  (kept the most recent by {globals().get('date_col','<no date>' )} when available; otherwise the first occurrence).\n",
        "\n",
        "**Impact**\n",
        "• Rows before → after: {globals().get('rows_before','?')} → {globals().get('rows_after','?')}.\n",
        "• Exact duplicates removed: {globals().get('exact_dups','?')}; ID-level duplicates removed: {globals().get('id_dups','?')}.\n",
        "• Dataset now has one row per customer, giving stable statistics and fair model training.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HHPbgT_PP_5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='data_cleaning_3_explanations', value=data_cleaning_3_explanations)"
      ],
      "metadata": {
        "id": "Pr9WS8jbP__p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B.n Fixing \"\\<describe_issue_here\\>\"\n",
        "\n",
        "> You can add more cells related to other issues in this section"
      ],
      "metadata": {
        "id": "VrxmkcJNLiFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bad date values (mixed formats, future dates, impossible birth dates) and also derives a robust tenure feature.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Start from the latest cleaned table; fall back gracefully\n",
        "base = None\n",
        "if isinstance(globals().get(\"X_clean3\"), pd.DataFrame) and not globals().get(\"X_clean3\").empty:\n",
        "    base = globals().get(\"X_clean3\")  # after B.2 outlier capping\n",
        "elif isinstance(globals().get(\"X_clean2\"), pd.DataFrame) and not globals().get(\"X_clean2\").empty:\n",
        "    base = globals().get(\"X_clean2\")\n",
        "elif isinstance(globals().get(\"X_clean1\"), pd.DataFrame) and not globals().get(\"X_clean1\").empty:\n",
        "    base = globals().get(\"X_clean1\")\n",
        "elif isinstance(globals().get(\"X_an\"), pd.DataFrame) and not globals().get(\"X_an\").empty:\n",
        "    base = globals().get(\"X_an\")\n",
        "elif isinstance(globals().get(\"X_a2\"), pd.DataFrame) and not globals().get(\"X_a2\").empty:\n",
        "    base = globals().get(\"X_a2\")\n",
        "elif isinstance(globals().get(\"X_a1\"), pd.DataFrame) and not globals().get(\"X_a1\").empty:\n",
        "    base = globals().get(\"X_a1\")\n",
        "elif isinstance(globals().get(\"customers_df\"), pd.DataFrame) and not globals().get(\"customers_df\").empty:\n",
        "    base = globals().get(\"customers_df\")\n",
        "\n",
        "assert base is not None and not base.empty, \"Run earlier prep steps before B.n.\"\n",
        "\n",
        "X_dt = base.copy()\n",
        "today = pd.Timestamp(\"today\").normalize()\n",
        "\n",
        "# 2) Candidate date columns (robust to different names/cases)\n",
        "candidates = [\n",
        "    \"birth_date\",\"BirthDate\",\n",
        "    \"created_at\",\"CreatedAt\",\"signup_date\",\"SignupDate\",\n",
        "    \"first_purchase_date\",\"FirstPurchaseDate\",\n",
        "    \"updated_at\",\"UpdatedAt\",\"last_update\",\"LastUpdate\",\n",
        "    \"order_date\",\"OrderDate\"\n",
        "]\n",
        "date_cols = [c for c in candidates if c in X_dt.columns]\n",
        "\n",
        "fixed_summary = {}\n",
        "\n",
        "# 3) Parse dates + fix clearly invalid values\n",
        "for c in date_cols:\n",
        "    s_raw = X_dt[c]\n",
        "    s = pd.to_datetime(s_raw, errors=\"coerce\", utc=False)        # tolerant parsing\n",
        "    before_na = s.isna().sum()\n",
        "\n",
        "    # (a) Future dates → NaT\n",
        "    s[s > today] = pd.NaT\n",
        "\n",
        "    # (b) Birth dates: reject <1900 or younger than 10 years old\n",
        "    if c.lower() in {\"birth_date\", \"birthdate\"}:\n",
        "        s[(s < pd.Timestamp(\"1900-01-01\")) | (s > today - pd.DateOffset(years=10))] = pd.NaT\n",
        "\n",
        "    # save back\n",
        "    X_dt[c] = s\n",
        "    fixed_summary[c] = {\"parsed_to_datetime\": True,\n",
        "                        \"new_NaT_added\": int(s.isna().sum() - before_na)}\n",
        "\n",
        "# 4) Derive tenure_days from the best available \"start\" date\n",
        "tenure_sources = [\"created_at\",\"CreatedAt\",\"signup_date\",\"SignupDate\",\"first_purchase_date\",\"FirstPurchaseDate\"]\n",
        "src = next((c for c in tenure_sources if c in X_dt.columns), None)\n",
        "\n",
        "if src is not None:\n",
        "    X_dt[\"tenure_days\"] = (today - X_dt[src]).dt.days\n",
        "    # Negative or NaN tenure (from NaT) → set to 0 for stability (optional)\n",
        "    X_dt[\"tenure_days\"] = X_dt[\"tenure_days\"].clip(lower=0).fillna(0).astype(\"int64\")\n",
        "else:\n",
        "    X_dt[\"tenure_days\"] = 0  # safe default if no start date exists\n",
        "\n",
        "# 5) Output\n",
        "X_cleanN = X_dt.copy()\n",
        "print(\"Date columns parsed/fixed:\", date_cols)\n",
        "print(\"Fix summary:\", fixed_summary)\n",
        "print(\"Tenure source:\", src, \"| nonzero tenure rows:\", int((X_cleanN['tenure_days']>0).sum()))\n",
        "display(X_cleanN.head(3))"
      ],
      "metadata": {
        "id": "WIL-U_qlLiMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85n-2YC-_c2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_cleaning_n_explanations = \"\"\"\n",
        "**Issue fixed:** Bad date values (mixed formats, future dates, impossible birth dates).\n",
        "\n",
        "**Why it matters**\n",
        "• Invalid dates break time-derived features (e.g., tenure, age), distort seasonality,\n",
        "  and can crash transformations that expect proper datetimes.\n",
        "\n",
        "**What I did**\n",
        "• Parsed all candidate date fields with tolerant coercion; set **future dates to NaT**.\n",
        "• For birth dates, removed values earlier than 1900 or implying age < 10 years.\n",
        "• Derived **tenure_days** from the earliest available start date (created/signup/first purchase).\n",
        "\n",
        "**Impact**\n",
        "• Clean, comparable dates; robust **tenure_days** feature for modelling/segmentation.\n",
        "• Reduces errors in downstream encoders and avoids misleading age/tenure statistics.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "octqRnEF_dAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='data_cleaning_n_explanations', value=data_cleaning_n_explanations)"
      ],
      "metadata": {
        "id": "WEy5hyRy_dQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## C. Feature Engineering"
      ],
      "metadata": {
        "id": "Z9jsWz-z5-XS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Y3Y4icAdoRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "# Create copy of datasets\n",
        "\n",
        "try:\n",
        "  training_df_eng = training_df_clean.copy()\n",
        "  validation_df_eng = validation_df_clean.copy()\n",
        "  testing_df_eng = testing_df_clean.copy()\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "viLWliu76Fm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C.1 New Feature \"\\<put_name_here\\>\"\n",
        "\n"
      ],
      "metadata": {
        "id": "imwl7ISs6O0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# income_per_dependent (and a stabilized log version)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Start from latest cleaned table; fall back gracefully\n",
        "base = None\n",
        "if isinstance(globals().get(\"X_cleanN\"), pd.DataFrame) and not globals().get(\"X_cleanN\").empty:\n",
        "    base = globals().get(\"X_cleanN\")\n",
        "elif isinstance(globals().get(\"X_clean3\"), pd.DataFrame) and not globals().get(\"X_clean3\").empty:\n",
        "    base = globals().get(\"X_clean3\")\n",
        "elif isinstance(globals().get(\"X_clean2\"), pd.DataFrame) and not globals().get(\"X_clean2\").empty:\n",
        "    base = globals().get(\"X_clean2\")\n",
        "elif isinstance(globals().get(\"X_clean1\"), pd.DataFrame) and not globals().get(\"X_clean1\").empty:\n",
        "    base = globals().get(\"X_clean1\")\n",
        "elif isinstance(globals().get(\"X_an\"), pd.DataFrame) and not globals().get(\"X_an\").empty:\n",
        "    base = globals().get(\"X_an\")\n",
        "elif isinstance(globals().get(\"X_a2\"), pd.DataFrame) and not globals().get(\"X_a2\").empty:\n",
        "    base = globals().get(\"X_a2\")\n",
        "elif isinstance(globals().get(\"X_a1\"), pd.DataFrame) and not globals().get(\"X_a1\").empty:\n",
        "    base = globals().get(\"X_a1\")\n",
        "elif isinstance(globals().get(\"customers_df\"), pd.DataFrame) and not globals().get(\"customers_df\").empty:\n",
        "    base = globals().get(\"customers_df\")\n",
        "\n",
        "assert base is not None and not base.empty, \"Run earlier prep steps before C.1.\"\n",
        "\n",
        "X_feat = base.copy()\n",
        "\n",
        "# 2) Ensure required inputs exist and are numeric\n",
        "inc_col = next((c for c in [\"annual_income\",\"income\",\"AnnualIncome\"] if c in X_feat.columns), None)\n",
        "dep_col = next((c for c in [\"number_dependents\",\"dependents\",\"NumberDependents\"] if c in X_feat.columns), None)\n",
        "assert inc_col is not None and dep_col is not None, \"Need annual income and number of dependents.\"\n",
        "\n",
        "X_feat[inc_col] = pd.to_numeric(X_feat[inc_col], errors=\"coerce\").fillna(0)\n",
        "X_feat[dep_col] = pd.to_numeric(X_feat[dep_col], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# 3) Compute income per dependent; add +1 in denominator to avoid division by zero\n",
        "X_feat[\"income_per_dependent\"] = X_feat[inc_col] / (1.0 + X_feat[dep_col])\n",
        "\n",
        "# 4) Log-transform to stabilize heavy right skew (log1p handles zeros)\n",
        "X_feat[\"log_income_per_dependent\"] = np.log1p(X_feat[\"income_per_dependent\"])\n",
        "\n",
        "# 5) Output for downstream steps\n",
        "print({\n",
        "    \"rows\": len(X_feat),\n",
        "    \"feature_head\": X_feat[[\"income_per_dependent\",\"log_income_per_dependent\"]].head(3).to_dict(\"records\")\n",
        "})\n",
        "display(X_feat[[inc_col, dep_col, \"income_per_dependent\", \"log_income_per_dependent\"]].head())"
      ],
      "metadata": {
        "id": "t5RVBQKW6mJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_engineering_1_explanations = \"\"\"\n",
        "**New feature:** `income_per_dependent` (+ `log_income_per_dependent`)\n",
        "\n",
        "**Why**\n",
        "• Captures effective spending capacity by spreading household income across dependents.\n",
        "• More interpretable than raw income and often more predictive for value/retention than income alone.\n",
        "\n",
        "**How**\n",
        "• Computed as {inc_col}/(1 + {dep_col}) to avoid divide-by-zero when dependents = 0.\n",
        "• Added a **log1p** version to reduce right-skew and help linear models.\n",
        "\n",
        "**Impact**\n",
        "• Provides a stable, business-meaningful signal for segmentation and downstream models.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "M-kYSZDfQFks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='feature_engineering_1_explanations', value=feature_engineering_1_explanations)"
      ],
      "metadata": {
        "id": "25d_SMvaQFr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C.3 New Feature \"\\<put_name_here\\>\"\n",
        "\n"
      ],
      "metadata": {
        "id": "jYxCnW3C6zJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tenure signals (tenure_years, tenure_bucket, recent_customer)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Start from latest table; fall back gracefully\n",
        "base = None\n",
        "if isinstance(globals().get(\"X_feat\"), pd.DataFrame) and not globals().get(\"X_feat\").empty:\n",
        "    base = globals().get(\"X_feat\")    # from C.1\n",
        "elif isinstance(globals().get(\"X_cleanN\"), pd.DataFrame) and not globals().get(\"X_cleanN\").empty:\n",
        "    base = globals().get(\"X_cleanN\")\n",
        "elif isinstance(globals().get(\"X_clean3\"), pd.DataFrame) and not globals().get(\"X_clean3\").empty:\n",
        "    base = globals().get(\"X_clean3\")\n",
        "elif isinstance(globals().get(\"X_clean2\"), pd.DataFrame) and not globals().get(\"X_clean2\").empty:\n",
        "    base = globals().get(\"X_clean2\")\n",
        "elif isinstance(globals().get(\"X_clean1\"), pd.DataFrame) and not globals().get(\"X_clean1\").empty:\n",
        "    base = globals().get(\"X_clean1\")\n",
        "elif isinstance(globals().get(\"X_an\"), pd.DataFrame) and not globals().get(\"X_an\").empty:\n",
        "    base = globals().get(\"X_an\")\n",
        "elif isinstance(globals().get(\"X_a2\"), pd.DataFrame) and not globals().get(\"X_a2\").empty:\n",
        "    base = globals().get(\"X_a2\")\n",
        "elif isinstance(globals().get(\"X_a1\"), pd.DataFrame) and not globals().get(\"X_a1\").empty:\n",
        "    base = globals().get(\"X_a1\")\n",
        "elif isinstance(globals().get(\"customers_df\"), pd.DataFrame) and not globals().get(\"customers_df\").empty:\n",
        "    base = globals().get(\"customers_df\")\n",
        "\n",
        "\n",
        "assert base is not None and not base.empty, \"Run earlier prep steps before C.3.\"\n",
        "\n",
        "X_ten = base.copy()\n",
        "today = pd.Timestamp(\"today\").normalize()\n",
        "\n",
        "# 2) Ensure tenure_days exists; derive from best available start date if needed\n",
        "if \"tenure_days\" not in X_ten.columns:\n",
        "    start_candidates = [\"created_at\",\"CreatedAt\",\"signup_date\",\"SignupDate\",\n",
        "                        \"first_purchase_date\",\"FirstPurchaseDate\"]\n",
        "    start_col = next((c for c in start_candidates if c in X_ten.columns), None)\n",
        "    if start_col:\n",
        "        X_ten[start_col] = pd.to_datetime(X_ten[start_col], errors=\"coerce\")\n",
        "        X_ten[\"tenure_days\"] = (today - X_ten[start_col]).dt.days\n",
        "    else:\n",
        "        X_ten[\"tenure_days\"] = 0  # safe default if no start date exists\n",
        "\n",
        "# Clean negatives/NaNs (can happen if bad dates slipped through)\n",
        "X_ten[\"tenure_days\"] = pd.to_numeric(X_ten[\"tenure_days\"], errors=\"coerce\").clip(lower=0).fillna(0).astype(int)\n",
        "\n",
        "# 3) New features:\n",
        "#    a) tenure_years (continuous, easy for linear models)\n",
        "X_ten[\"tenure_years\"] = X_ten[\"tenure_days\"] / 365.25\n",
        "\n",
        "#    b) tenure_bucket (categorical bands for business reporting)\n",
        "bins  = [-1, 90, 180, 365, 1095, np.inf]\n",
        "labels = [\"≤3m\", \"3–6m\", \"6–12m\", \"1–3y\", \"≥3y\"]\n",
        "X_ten[\"tenure_bucket\"] = pd.cut(X_ten[\"tenure_days\"], bins=bins, labels=labels)\n",
        "\n",
        "#    c) recent_customer flag (useful for churn/activation features)\n",
        "X_ten[\"recent_customer\"] = np.where(X_ten[\"tenure_days\"] <= 90, 1, 0).astype(int)\n",
        "\n",
        "# 4) Output for downstream steps\n",
        "print({\n",
        "    \"rows\": len(X_ten),\n",
        "    \"example\": X_ten[[\"tenure_days\",\"tenure_years\",\"tenure_bucket\",\"recent_customer\"]].head(3).to_dict(\"records\")\n",
        "})\n",
        "display(X_ten[[\"tenure_days\",\"tenure_years\",\"tenure_bucket\",\"recent_customer\"]].head())"
      ],
      "metadata": {
        "id": "y03X8eeW6zbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_engineering_2_explanations = \"\"\"\n",
        "**New features:** `tenure_years`, `tenure_bucket`, `recent_customer`\n",
        "\n",
        "**Why**\n",
        "• Tenure captures relationship depth and is often predictive of spend, retention, and support demand.\n",
        "• Buckets provide an easy business lens (≤3m, 3–6m, 6–12m, 1–3y, ≥3y) for reporting and stratified evaluation.\n",
        "\n",
        "**How**\n",
        "• Derived `tenure_days` from created/signup/first-purchase date (fallback if missing), then:\n",
        "  – `tenure_years` = tenure_days / 365.25 (continuous),\n",
        "  – `tenure_bucket` = categorical bands,\n",
        "  – `recent_customer` = 1 if tenure ≤ 90 days, else 0.\n",
        "\n",
        "**Impact**\n",
        "• Adds both interpretable segments and model-friendly numeric signal; useful for targeting onboarding vs. mature customers.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UVtR91G3QLVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='feature_engineering_2_explanations', value=feature_engineering_2_explanations)"
      ],
      "metadata": {
        "id": "PMH2cyXzQLYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C.4 New Feature \"\\<put_name_here\\>\"\n",
        "\n"
      ],
      "metadata": {
        "id": "gWF4jvXb62kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# socioeconomic_score (+ band)\n",
        "# Combines income strength, education level, and homeownership into one interpretable index.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Start from latest table; fall back gracefully\n",
        "base = None\n",
        "if isinstance(globals().get(\"X_ten\"), pd.DataFrame) and not globals().get(\"X_ten\").empty:\n",
        "    base = globals().get(\"X_ten\")    # from C.3\n",
        "elif isinstance(globals().get(\"X_feat\"), pd.DataFrame) and not globals().get(\"X_feat\").empty:\n",
        "    base = globals().get(\"X_feat\")    # from C.1\n",
        "elif isinstance(globals().get(\"X_cleanN\"), pd.DataFrame) and not globals().get(\"X_cleanN\").empty:\n",
        "    base = globals().get(\"X_cleanN\")  # from B.n\n",
        "elif isinstance(globals().get(\"X_clean3\"), pd.DataFrame) and not globals().get(\"X_clean3\").empty:\n",
        "    base = globals().get(\"X_clean3\")  # from B.3\n",
        "elif isinstance(globals().get(\"X_clean2\"), pd.DataFrame) and not globals().get(\"X_clean2\").empty:\n",
        "    base = globals().get(\"X_clean2\")  # from B.2\n",
        "elif isinstance(globals().get(\"X_clean1\"), pd.DataFrame) and not globals().get(\"X_clean1\").empty:\n",
        "    base = globals().get(\"X_clean1\")  # from B.1\n",
        "elif isinstance(globals().get(\"X_an\"), pd.DataFrame) and not globals().get(\"X_an\").empty:\n",
        "    base = globals().get(\"X_an\")    # from A.n\n",
        "elif isinstance(globals().get(\"X_a2\"), pd.DataFrame) and not globals().get(\"X_a2\").empty:\n",
        "    base = globals().get(\"X_a2\")    # from A.2\n",
        "elif isinstance(globals().get(\"X_a1\"), pd.DataFrame) and not globals().get(\"X_a1\").empty:\n",
        "    base = globals().get(\"X_a1\")   # from A.1\n",
        "elif isinstance(globals().get(\"customers_df\"), pd.DataFrame) and not globals().get(\"customers_df\").empty:\n",
        "    base = globals().get(\"customers_df\") # loaded from file\n",
        "\n",
        "assert base is not None and not base.empty, \"Run earlier prep steps before C.4.\"\n",
        "\n",
        "X_soc = base.copy()\n",
        "\n",
        "# 2) Column names (robust)\n",
        "inc_col = next((c for c in [\"annual_income\",\"income\",\"AnnualIncome\"] if c in X_soc.columns), None)\n",
        "edu_col = next((c for c in [\"education_level\",\"EducationLevel\",\"education\"] if c in X_soc.columns), None)\n",
        "home_col= next((c for c in [\"homeowner\",\"Homeowner\",\"housing_status\",\"HousingStatus\"] if c in X_soc.columns), None)\n",
        "assert inc_col is not None, \"Need an income column (e.g., annual_income).\"\n",
        "# education/homeowner are optional; if absent we default to neutral values below.\n",
        "\n",
        "# 3) Normalise inputs\n",
        "X_soc[inc_col] = pd.to_numeric(X_soc[inc_col], errors=\"coerce\").fillna(0)\n",
        "\n",
        "def _norm_text(s):\n",
        "    return s.astype(str).str.strip().str.lower()\n",
        "\n",
        "# education → ordinal 0..4\n",
        "edu_map = {\n",
        "    \"primary\":0, \"elementary\":0,\n",
        "    \"secondary\":1, \"high school\":1, \"highschool\":1,\n",
        "    \"diploma\":2, \"certificate\":2, \"trade\":2,\n",
        "    \"bachelor\":3, \"bachelors\":3, \"undergraduate\":3, \"degree\":3,\n",
        "    \"master\":4, \"masters\":4, \"postgraduate\":4, \"doctorate\":4, \"phd\":4\n",
        "}\n",
        "if edu_col is not None:\n",
        "    edu_norm = _norm_text(X_soc[edu_col])\n",
        "    X_soc[\"edu_level_num\"] = edu_norm.map(edu_map)\n",
        "    X_soc[\"edu_level_num\"] = X_soc[\"edu_level_num\"].fillna(X_soc[\"edu_level_num\"].median() if X_soc[\"edu_level_num\"].notna().any() else 2)\n",
        "else:\n",
        "    X_soc[\"edu_level_num\"] = 2  # neutral if education missing\n",
        "\n",
        "# homeowner → 0/1\n",
        "true_like  = {\"y\",\"yes\",\"true\",\"owner\",\"own\",\"1\"}\n",
        "if home_col is not None:\n",
        "    home_norm = _norm_text(X_soc[home_col])\n",
        "    X_soc[\"homeowner_flag\"] = home_norm.isin(true_like).astype(int)\n",
        "else:\n",
        "    X_soc[\"homeowner_flag\"] = 0  # neutral fallback\n",
        "\n",
        "# 4) Income percentile (0..1) for scale-free combination\n",
        "income_pct = X_soc[inc_col].rank(pct=True, method=\"average\").fillna(0)\n",
        "\n",
        "# 5) Composite index (weights sum to 1 for interpretability)\n",
        "#    income has strongest weight; education moderate; homeowner small but useful.\n",
        "X_soc[\"socioeconomic_score\"] = (\n",
        "    0.6 * income_pct +\n",
        "    0.3 * (X_soc[\"edu_level_num\"] / 4.0) +\n",
        "    0.1 * X_soc[\"homeowner_flag\"]\n",
        ")\n",
        "\n",
        "# 6) Band version for business reporting\n",
        "try:\n",
        "    X_soc[\"socioeconomic_band\"] = pd.qcut(X_soc[\"socioeconomic_score\"], q=4, labels=[\"Low\",\"Mid-Low\",\"Mid-High\",\"High\"])\n",
        "except Exception:\n",
        "    # fallback if too few unique values\n",
        "    X_soc[\"socioeconomic_band\"] = pd.cut(X_soc[\"socioeconomic_score\"], bins=[-np.inf,0.25,0.5,0.75,np.inf],\n",
        "                                         labels=[\"Low\",\"Mid-Low\",\"Mid-High\",\"High\"])\n",
        "\n",
        "# 7) Output preview\n",
        "print({\n",
        "    \"rows\": len(X_soc),\n",
        "    \"example\": X_soc[[inc_col,\"edu_level_num\",\"homeowner_flag\",\"socioeconomic_score\",\"socioeconomic_band\"]].head(3).to_dict(\"records\")\n",
        "})\n",
        "display(X_soc[[inc_col,\"edu_level_num\",\"homeowner_flag\",\"socioeconomic_score\",\"socioeconomic_band\"]].head())"
      ],
      "metadata": {
        "id": "dfXFMHqw62r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9iuqF2MBLwKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_engineering_3_explanations = \"\"\"\n",
        "**New features:** `socioeconomic_score` and `socioeconomic_band`\n",
        "\n",
        "**Why**\n",
        "• Combines three interpretable signals—income strength, education level, and homeownership—into one index.\n",
        "• Useful for segmentation (targeting offers) and often predictive of value/retention.\n",
        "\n",
        "**How**\n",
        "• Income converted to a **percentile rank** (0–1) so it’s scale-free.\n",
        "• Education mapped to an ordinal 0–4 (Primary→Doctorate).\n",
        "• Homeownership converted to a binary flag.\n",
        "• Weighted blend: 0.6·income + 0.3·education + 0.1·homeowner; then bucketed into **Low / Mid-Low / Mid-High / High**.\n",
        "\n",
        "**Impact**\n",
        "• Produces a compact, business-friendly signal while remaining transparent and easy to justify.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ai9-L0dnQPvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='feature_engineering_3_explanations', value=feature_engineering_3_explanations)"
      ],
      "metadata": {
        "id": "okaLOh0SQP1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C.n Fixing \"\\<describe_issue_here\\>\"\n",
        "\n",
        "> You can add more cells related to new features in this section"
      ],
      "metadata": {
        "id": "l9Eh63GQLw4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-engineering issues (NaN/Inf, wrong dtypes, invalid ranges)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Start from the latest engineered table; fall back gracefully\n",
        "base = None\n",
        "if isinstance(globals().get(\"X_soc\"), pd.DataFrame) and not globals().get(\"X_soc\").empty:\n",
        "    base = globals().get(\"X_soc\")    # from C.4\n",
        "elif isinstance(globals().get(\"X_ten\"), pd.DataFrame) and not globals().get(\"X_ten\").empty:\n",
        "    base = globals().get(\"X_ten\")    # from C.3\n",
        "elif isinstance(globals().get(\"X_feat\"), pd.DataFrame) and not globals().get(\"X_feat\").empty:\n",
        "    base = globals().get(\"X_feat\")    # from C.1\n",
        "elif isinstance(globals().get(\"X_cleanN\"), pd.DataFrame) and not globals().get(\"X_cleanN\").empty:\n",
        "    base = globals().get(\"X_cleanN\")  # from B.n\n",
        "elif isinstance(globals().get(\"X_clean3\"), pd.DataFrame) and not globals().get(\"X_clean3\").empty:\n",
        "    base = globals().get(\"X_clean3\")  # from B.3\n",
        "elif isinstance(globals().get(\"X_clean2\"), pd.DataFrame) and not globals().get(\"X_clean2\").empty:\n",
        "    base = globals().get(\"X_clean2\")  # from B.2\n",
        "elif isinstance(globals().get(\"X_clean1\"), pd.DataFrame) and not globals().get(\"X_clean1\").empty:\n",
        "    base = globals().get(\"X_clean1\")  # from B.1\n",
        "elif isinstance(globals().get(\"X_an\"), pd.DataFrame) and not globals().get(\"X_an\").empty:\n",
        "    base = globals().get(\"X_an\")    # from A.n\n",
        "elif isinstance(globals().get(\"X_a2\"), pd.DataFrame) and not globals().get(\"X_a2\").empty:\n",
        "    base = globals().get(\"X_a2\")    # from A.2\n",
        "elif isinstance(globals().get(\"X_a1\"), pd.DataFrame) and not globals().get(\"X_a1\").empty:\n",
        "    base = globals().get(\"X_a1\")   # from A.1\n",
        "elif isinstance(globals().get(\"customers_df\"), pd.DataFrame) and not globals().get(\"customers_df\").empty:\n",
        "    base = globals().get(\"customers_df\") # loaded from file\n",
        "\n",
        "\n",
        "assert base is not None and not base.empty, \"Run earlier C.* steps before C.n.\"\n",
        "\n",
        "X_fix = base.copy()\n",
        "\n",
        "# 2) Engineered columns we may have created earlier (only keep those that exist)\n",
        "num_candidates = [\n",
        "    \"income_per_dependent\", \"log_income_per_dependent\",\n",
        "    \"tenure_days\", \"tenure_years\",\n",
        "    \"socioeconomic_score\"\n",
        "]\n",
        "cat_candidates = [\"tenure_bucket\", \"socioeconomic_band\"]\n",
        "bin_candidates = [\"recent_customer\"]\n",
        "\n",
        "num_cols = [c for c in num_candidates if c in X_fix.columns]\n",
        "cat_cols = [c for c in cat_candidates if c in X_fix.columns]\n",
        "bin_cols = [c for c in bin_candidates if c in X_fix.columns]\n",
        "\n",
        "# 3) Replace Inf/−Inf with NaN, then impute numerics with median\n",
        "for c in num_cols:\n",
        "    s = pd.to_numeric(X_fix[c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
        "    s = s.fillna(s.median() if s.notna().any() else 0)\n",
        "    # sensible range constraints\n",
        "    if c in {\"income_per_dependent\",\"tenure_days\",\"tenure_years\",\"log_income_per_dependent\"}:\n",
        "        s = s.clip(lower=0)\n",
        "    # keep as float for safety\n",
        "    X_fix[c] = s.astype(float)\n",
        "\n",
        "# 4) Ensure binary flags are 0/1 integers\n",
        "for c in bin_cols:\n",
        "    s = pd.to_numeric(X_fix[c], errors=\"coerce\").fillna(0)\n",
        "    X_fix[c] = np.where(s > 0, 1, 0).astype(int)\n",
        "\n",
        "# 5) Ensure engineered categoricals are category dtype and mode-impute\n",
        "for c in cat_cols:\n",
        "    col = X_fix[c].astype(\"string\").str.strip()\n",
        "    if col.isna().any():\n",
        "        col = col.fillna(col.mode().iloc[0] if col.mode().size else \"Unknown\")\n",
        "    X_fix[c] = col.astype(\"category\")\n",
        "\n",
        "# 6) Output for downstream steps\n",
        "X_fixed_engineered = X_fix.copy()\n",
        "print({\n",
        "    \"rows\": len(X_fixed_engineered),\n",
        "    \"fixed_numeric\": num_cols,\n",
        "    \"fixed_binary\": bin_cols,\n",
        "    \"fixed_categoricals\": cat_cols\n",
        "})\n",
        "display(X_fixed_engineered.head(3))"
      ],
      "metadata": {
        "id": "SZjz6x6vLw-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8GTtThsR_Nhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "feature_engineering_n_explanations = \"\"\"\n",
        "**Issue fixed:** Post-engineering artefacts — NaN/Inf values, wrong dtypes, and invalid ranges.\n",
        "\n",
        "**Why it matters**\n",
        "• Newly created features often contain {np.inf}/−{np.inf} (e.g., logs) or NaNs from division;\n",
        "  these break scalers/encoders and can bias model training.\n",
        "\n",
        "**What I did**\n",
        "• Replaced Inf/−Inf with NaN and **median-imputed** engineered numerics; clipped to sensible ranges (≥0 for tenure/income features).\n",
        "• Forced binary features (e.g., `recent_customer`) to **0/1 int**.\n",
        "• Cast engineered buckets (e.g., `tenure_bucket`, `socioeconomic_band`) to **category** and mode-imputed.\n",
        "\n",
        "**Impact**\n",
        "• Clean, consistent engineered features with stable dtypes; ready for encoding/scaling and downstream modelling.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GCgD5TwA_NrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='feature_engineering_n_explanations', value=feature_engineering_n_explanations)"
      ],
      "metadata": {
        "id": "q-H6x7Tf_N0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0S5LSAcjkvP"
      },
      "source": [
        "---\n",
        "## D. Data Preparation for Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D.1 Split Datasets\n"
      ],
      "metadata": {
        "id": "JOCutkG6k3GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust 70/15/15 split (no ambiguous truth-values; stratify only when valid)\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_FRAC = 0.15\n",
        "VAL_FRAC  = 0.15           # overall validation share\n",
        "VAL_WITHIN = VAL_FRAC / (1 - TEST_FRAC)   # ≈ 0.1765 of the train+val pool\n",
        "\n",
        "# 1) pick the latest prepared table available\n",
        "candidates = [\n",
        "    \"X_fixed_engineered\",\"X_soc\",\"X_ten\",\"X_feat\",\n",
        "    \"X_cleanN\",\"X_clean3\",\"X_clean2\",\"X_clean1\",\n",
        "    \"X_an\",\"X_a2\",\"X_a1\",\"customers\",\"sales_2022\",\"sales_2021\",\"sales_2020\"\n",
        "]\n",
        "base = None; src_name = None\n",
        "for n in candidates:\n",
        "    obj = globals().get(n)\n",
        "    if isinstance(obj, pd.DataFrame) and not obj.empty:\n",
        "        base, src_name = obj.copy(), n\n",
        "        break\n",
        "if base is None:\n",
        "    raise RuntimeError(\"No non-empty DataFrame found. Load/prepare data before splitting.\")\n",
        "\n",
        "# 2) detect target (optional)\n",
        "y_col = (globals().get(\"y_col\") or\n",
        "         next((c for c in [\"credit_rating\",\"churn\",\"target\",\"label\",\"segment\"] if c in base.columns), None))\n",
        "\n",
        "# exclude ID/date-like columns from features\n",
        "id_like   = [c for c in [\"CustomerID\",\"customer_id\",\"CustomerKey\",\"customer_key\",\"id\",\"ID\"] if c in base.columns]\n",
        "date_like = [c for c in base.columns if \"date\" in c.lower()]\n",
        "exclude   = set(id_like + date_like + ([y_col] if y_col is not None else []))\n",
        "\n",
        "X_all = base.drop(columns=list(exclude), errors=\"ignore\")\n",
        "y_all = None\n",
        "if y_col is not None:\n",
        "    y_all = base[y_col]\n",
        "    # if someone made target a single-column DataFrame, convert to Series\n",
        "    if isinstance(y_all, pd.DataFrame) and y_all.shape[1] == 1:\n",
        "        y_all = y_all.iloc[:, 0]\n",
        "\n",
        "# helper to decide stratification only when valid\n",
        "def _make_strata(y: pd.Series):\n",
        "    if not isinstance(y, pd.Series):\n",
        "        return None\n",
        "    is_class = (y.dtype == \"object\") or (str(y.dtype) == \"category\") or (y.nunique(dropna=True) <= 20)\n",
        "    if not is_class:\n",
        "        return None\n",
        "    vc = y.value_counts()\n",
        "    # need at least 3 per class to safely split 70/15/15\n",
        "    return y.astype(str) if (len(vc) > 0 and vc.min() >= 3) else None\n",
        "\n",
        "# 3) split\n",
        "if y_all is None:\n",
        "    # unsupervised: split X only\n",
        "    X_trv, X_te = train_test_split(X_all, test_size=TEST_FRAC, random_state=RANDOM_STATE, shuffle=True)\n",
        "    X_tr,  X_va = train_test_split(X_trv, test_size=VAL_WITHIN,  random_state=RANDOM_STATE, shuffle=True)\n",
        "    y_tr = y_va = y_te = None\n",
        "    problem = \"unsupervised\"\n",
        "else:\n",
        "    # supervised: include y; stratify when appropriate\n",
        "    strata = _make_strata(y_all)\n",
        "    X_trv, X_te, y_trv, y_te = train_test_split(\n",
        "        X_all, y_all, test_size=TEST_FRAC, random_state=RANDOM_STATE, shuffle=True, stratify=strata\n",
        "    )\n",
        "    strata_trv = _make_strata(y_trv)\n",
        "    X_tr, X_va, y_tr, y_va = train_test_split(\n",
        "        X_trv, y_trv, test_size=VAL_WITHIN, random_state=RANDOM_STATE, shuffle=True, stratify=strata_trv\n",
        "    )\n",
        "    problem = \"classification\" if strata is not None else \"regression\"\n",
        "\n",
        "# 4) expose to globals for later cells\n",
        "globals().update(dict(X_tr=X_tr, X_va=X_va, X_te=X_te, y_tr=y_tr, y_va=y_va, y_te=y_te))\n",
        "\n",
        "print({\n",
        "    \"base_source\": src_name,\n",
        "    \"problem\": problem,\n",
        "    \"target\": y_col,\n",
        "    \"excluded_cols\": sorted(list(exclude)),\n",
        "    \"X_train\": X_tr.shape, \"X_valid\": X_va.shape, \"X_test\": X_te.shape,\n",
        "    \"y_present\": y_all is not None\n",
        "})\n"
      ],
      "metadata": {
        "id": "UmPVLaNulC1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <Student to fill this section>\n",
        "data_splitting_explanations = \"\"\"\n",
        "Choosing the appropriate strategy for splitting the data will help ensure that your machine learning model generalizes well on data it has not seen, and that the model performance evaluation is as realistic as possible. Now, considering this customer dataset, common strategies include:\n",
        "\n",
        "Random Split: This is the most straightforward approach, where data gets randomly allocated across training, validation, and testing sets. For instance, it could be 70% training, 15% validation, and 15% test. It is appropriate when data is assumed to be i.i.d. and there are no specific temporal or grouping structures to be kept by implementing a random split.\n",
        "\n",
        "Stratified Split: If a target variable for some supervised task is introduced, a stratified split will ensure the proportion of the target variable categories is maintained in each split-that is, train, validation, and test. This is important for a classification problem with classes that are imbalanced so that splits where some classes are poorly represented or absent are avoided; hence, the cell includes logic to attempt stratification if a suitable target variable is present.\n",
        "\n",
        "Time-based Split: In the case of a task that involves predicting future customer behavior, where the data contains a time component (e.g., date of customer acquisition, last transaction date), splitting should be based on time, such that the model is trained on data occurring up until a specified date and tested on data post-occurrence of that date. This dataset includes date information such as birth_date.\n",
        "\n",
        "Grouped Split: When the data have natural groupings (for example, customer records associated with the same household, or sales data associated with the same sales territory), and you want to test the model's ability to generalize to completely new groups, then the split should be done such that samples from a specific group are kept within the same set, either in the train, validation, or test set. This dataset contains customer_id and territorykey, which could be used for grouping. Because the problem at hand is considered unsupervised, this simple random split is a reasonable default. If you later define a supervised task, you will want to consider whether stratification, time-based, or grouped splitting would better allow proper evaluation of your model's performance on that particular task.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gZFFgktrlC7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h3\", key='data_splitting_explanations', value=data_splitting_explanations)"
      ],
      "metadata": {
        "id": "nY914h0klDAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D.2 Data Transformation <put_name_here>\n"
      ],
      "metadata": {
        "id": "RKqt6BN6csNC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PnGwsy2hez4"
      },
      "outputs": [],
      "source": [
        "# numeric(impute+scale) + categorical\n",
        "# Produces X_tr_tf, X_va_tf, X_te_tf as DataFrames with feature names.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# 1) Inputs from D.1\n",
        "assert \"X_tr\" in globals() and \"X_va\" in globals() and \"X_te\" in globals(), \\\n",
        "    \"Run the D.1 splitting cell first to create X_tr/X_va/X_te.\"\n",
        "\n",
        "Xtr_raw, Xva_raw, Xte_raw = X_tr.copy(), X_va.copy(), X_te.copy()\n",
        "\n",
        "# 2) Identify column types\n",
        "num_cols = [c for c in Xtr_raw.columns if pd.api.types.is_numeric_dtype(Xtr_raw[c])]\n",
        "cat_cols = [c for c in Xtr_raw.columns if c not in num_cols]\n",
        "\n",
        "# 3) Define per-type pipelines\n",
        "num_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\",  StandardScaler(with_mean=True, with_std=True)),\n",
        "])\n",
        "\n",
        "cat_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ohe\",     OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "])\n",
        "\n",
        "# 4) ColumnTransformer that applies the two pipes\n",
        "preproc = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", num_pipe, num_cols),\n",
        "        (\"cat\", cat_pipe, cat_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# 5) Fit on TRAIN ONLY, then transform all splits\n",
        "preproc.fit(Xtr_raw)\n",
        "\n",
        "def _to_df(arr, transformer, num_cols, cat_cols):\n",
        "    # Build feature names from the fitted transformer\n",
        "    try:\n",
        "        names = transformer.get_feature_names_out()\n",
        "    except Exception:\n",
        "        # Fallback: compose manually if needed\n",
        "        names = list(num_cols)\n",
        "        if cat_cols:\n",
        "            ohe = transformer.named_transformers_[\"cat\"][\"ohe\"]\n",
        "            cat_names = ohe.get_feature_names_out(cat_cols).tolist()\n",
        "            names += cat_names\n",
        "    return pd.DataFrame(arr, columns=names, index=None)\n",
        "\n",
        "X_tr_tf = _to_df(preproc.transform(Xtr_raw), preproc, num_cols, cat_cols)\n",
        "X_va_tf = _to_df(preproc.transform(Xva_raw), preproc, num_cols, cat_cols)\n",
        "X_te_tf = _to_df(preproc.transform(Xte_raw), preproc, num_cols, cat_cols)\n",
        "\n",
        "# 6) Expose objects for downstream notebooks\n",
        "globals().update(dict(preproc=preproc, X_tr_tf=X_tr_tf, X_va_tf=X_va_tf, X_te_tf=X_te_tf))\n",
        "\n",
        "print({\n",
        "    \"num_cols\": len(num_cols),\n",
        "    \"cat_cols\": len(cat_cols),\n",
        "    \"X_tr_tf\": X_tr_tf.shape,\n",
        "    \"X_va_tf\": X_va_tf.shape,\n",
        "    \"X_te_tf\": X_te_tf.shape\n",
        "})\n",
        "display(X_tr_tf.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_transformation_1_explanations = \"\"\"\n",
        "**Transformations applied:**\n",
        "• **Numeric**: median imputation → standard scaling. Median is robust to outliers; scaling centers features and\n",
        "  gives comparable magnitude, helping regularised linear models and distance-based learners.\n",
        "• **Categorical**: most-frequent imputation → one-hot encoding with `handle_unknown='ignore'` so unseen levels at\n",
        "  validation/test don’t break the pipeline.\n",
        "\n",
        "**Why this is appropriate here**\n",
        "• The dataset mixes numeric and categorical fields (per EDA/Preparation). This pipeline preserves signal while\n",
        "  preventing leakage (fit on train only) and keeps evaluation fair across splits.\n",
        "\n",
        "**Outputs**\n",
        "• `X_tr_tf`, `X_va_tf`, `X_te_tf` are pandas DataFrames with explicit feature names, ready for the Baseline/Model notebooks.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "or7RgXjwQaAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='data_transformation_1_explanations', value=data_transformation_1_explanations)"
      ],
      "metadata": {
        "id": "f7EzJD6JQaF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D.3 Data Transformation <put_name_here>"
      ],
      "metadata": {
        "id": "Nl12P1VIdFGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numeric: median impute -> QuantileTransformer(output='normal')\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# 1) Get raw splits from D.1\n",
        "assert all(k in globals() for k in [\"X_tr\",\"X_va\",\"X_te\"]), \"Run D.1 first.\"\n",
        "Xtr_raw, Xva_raw, Xte_raw = X_tr.copy(), X_va.copy(), X_te.copy()\n",
        "\n",
        "# 2) Column types from train\n",
        "num_cols = [c for c in Xtr_raw.columns if pd.api.types.is_numeric_dtype(Xtr_raw[c])]\n",
        "cat_cols = [c for c in Xtr_raw.columns if c not in num_cols]\n",
        "\n",
        "# 3) Define per-type pipelines (Yeo-Johnson-like, rank-based Gaussianisation)\n",
        "num_q_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"qtf\",     QuantileTransformer(output_distribution=\"normal\", subsample=100000, random_state=42)),\n",
        "])\n",
        "\n",
        "cat_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ohe\",     OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "])\n",
        "\n",
        "# 4) ColumnTransformer combining both\n",
        "preproc_q = ColumnTransformer(\n",
        "    [(\"num\", num_q_pipe, num_cols),\n",
        "     (\"cat\", cat_pipe,   cat_cols)],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# 5) Fit on TRAIN ONLY, transform all splits\n",
        "preproc_q.fit(Xtr_raw)\n",
        "\n",
        "def _to_df(arr, transformer, num_cols, cat_cols):\n",
        "    try:\n",
        "        names = transformer.get_feature_names_out()\n",
        "    except Exception:\n",
        "        # Fallback: compose OHE names manually if needed\n",
        "        names = list(num_cols)\n",
        "        if cat_cols:\n",
        "            ohe = transformer.named_transformers_[\"cat\"][\"ohe\"]\n",
        "            names += ohe.get_feature_names_out(cat_cols).tolist()\n",
        "    return pd.DataFrame(arr, columns=names)\n",
        "\n",
        "X_tr_tf_q = _to_df(preproc_q.transform(Xtr_raw), preproc_q, num_cols, cat_cols)\n",
        "X_va_tf_q = _to_df(preproc_q.transform(Xva_raw), preproc_q, num_cols, cat_cols)\n",
        "X_te_tf_q = _to_df(preproc_q.transform(Xte_raw), preproc_q, num_cols, cat_cols)\n",
        "\n",
        "# 6) Expose for downstream use\n",
        "globals().update(dict(preproc_q=preproc_q,\n",
        "                      X_tr_tf_q=X_tr_tf_q, X_va_tf_q=X_va_tf_q, X_te_tf_q=X_te_tf_q))\n",
        "\n",
        "print({\n",
        "    \"num_cols\": len(num_cols),\n",
        "    \"cat_cols\": len(cat_cols),\n",
        "    \"X_tr_tf_q\": X_tr_tf_q.shape,\n",
        "    \"X_va_tf_q\": X_va_tf_q.shape,\n",
        "    \"X_te_tf_q\": X_te_tf_q.shape\n",
        "})\n",
        "display(X_tr_tf_q.head(3))"
      ],
      "metadata": {
        "id": "4LLUCGNPQhO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0FVb16jUL-Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_transformation_2_explanations = \"\"\"\n",
        "**What I did**\n",
        "• Built an alternative preprocessing pipeline:\n",
        "  – **Numeric** → median imputation then **QuantileTransformer(output='normal')** to Gaussianise skewed features.\n",
        "  – **Categorical** → most-frequent imputation then One-Hot Encoding with unknowns ignored.\n",
        "• Fitted the transformers **only on the training split** and applied to validation/test.\n",
        "\n",
        "**Why**\n",
        "• Many numeric variables in retail data are right-skewed (spend, frequency, amounts). Mapping them to a\n",
        "  quasi-normal distribution can improve linear models and distance-based learners by reducing the influence of outliers.\n",
        "• Keeping the same categorical treatment ensures comparability with D.2 while isolating the impact of the numeric transform.\n",
        "\n",
        "**Outputs**\n",
        "• `X_tr_tf_q`, `X_va_tf_q`, `X_te_tf_q` — transformed DataFrames with feature names.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DGuIGxuFQhZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='data_transformation_2_explanations', value=data_transformation_2_explanations)"
      ],
      "metadata": {
        "id": "0fWVr_nAQhkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D.4 Data Transformation <put_name_here>\n"
      ],
      "metadata": {
        "id": "nw8W-JpHdGlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Numeric: median impute -> RobustScaler   (robust to outliers)\n",
        "# Categorical: most-frequent impute -> group rare levels -> OneHotEncoder\n",
        "# Outputs: X_tr_tf_r, X_va_tf_r, X_te_tf_r\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# 1) Raw splits from D.1\n",
        "assert all(k in globals() for k in [\"X_tr\",\"X_va\",\"X_te\"]), \"Run D.1 first.\"\n",
        "Xtr_raw, Xva_raw, Xte_raw = X_tr.copy(), X_va.copy(), X_te.copy()\n",
        "\n",
        "# 2) Column types from train\n",
        "num_cols = [c for c in Xtr_raw.columns if pd.api.types.is_numeric_dtype(Xtr_raw[c])]\n",
        "cat_cols = [c for c in Xtr_raw.columns if c not in num_cols]\n",
        "\n",
        "# 3) Small helper: group rare categories by frequency learned on TRAIN\n",
        "class RareGrouper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, min_freq=0.01):  # fraction of rows; e.g., 1%\n",
        "        self.min_freq = float(min_freq)\n",
        "        self.keep_levels_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = pd.DataFrame(X).copy()\n",
        "        for col in X.columns:\n",
        "            vc = X[col].astype(\"string\").value_counts(normalize=True, dropna=False)\n",
        "            self.keep_levels_[col] = set(vc[vc >= self.min_freq].index.tolist())\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = pd.DataFrame(X).copy()\n",
        "        for col in X.columns:\n",
        "            keep = self.keep_levels_.get(col, set())\n",
        "            X[col] = X[col].astype(\"string\").where(X[col].astype(\"string\").isin(keep), other=\"Other\")\n",
        "        return X\n",
        "\n",
        "# 4) Define per-type pipelines\n",
        "num_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\",  RobustScaler(with_centering=True, with_scaling=True)),\n",
        "])\n",
        "\n",
        "cat_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"rare\",    RareGrouper(min_freq=0.01)),                      # group train-rare levels as \"Other\"\n",
        "    (\"ohe\",     OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "])\n",
        "\n",
        "# 5) ColumnTransformer combining both\n",
        "preproc_r = ColumnTransformer(\n",
        "    [(\"num\", num_pipe, num_cols),\n",
        "     (\"cat\", cat_pipe,   cat_cols)],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# 6) Fit on TRAIN ONLY, transform all splits\n",
        "preproc_r.fit(Xtr_raw)\n",
        "\n",
        "def _to_df(arr, transformer, num_cols, cat_cols):\n",
        "    try:\n",
        "        names = transformer.get_feature_names_out()\n",
        "    except Exception:\n",
        "        names = list(num_cols)\n",
        "        if cat_cols:\n",
        "            ohe = transformer.named_transformers_[\"cat\"][\"ohe\"]\n",
        "            names += ohe.get_feature_names_out(cat_cols).tolist()\n",
        "    return pd.DataFrame(arr, columns=names)\n",
        "\n",
        "X_tr_tf_r = _to_df(preproc_r.transform(Xtr_raw), preproc_r, num_cols, cat_cols)\n",
        "X_va_tf_r = _to_df(preproc_r.transform(Xva_raw), preproc_r, num_cols, cat_cols)\n",
        "X_te_tf_r = _to_df(preproc_r.transform(Xte_raw), preproc_r, num_cols, cat_cols)\n",
        "\n",
        "# 7) Expose for downstream notebooks\n",
        "globals().update(dict(preproc_r=preproc_r,\n",
        "                      X_tr_tf_r=X_tr_tf_r, X_va_tf_r=X_va_tf_r, X_te_tf_r=X_te_tf_r))\n",
        "\n",
        "print({\n",
        "    \"num_cols\": len(num_cols),\n",
        "    \"cat_cols\": len(cat_cols),\n",
        "    \"X_tr_tf_r\": X_tr_tf_r.shape,\n",
        "    \"X_va_tf_r\": X_va_tf_r.shape,\n",
        "    \"X_te_tf_r\": X_te_tf_r.shape\n",
        "})\n",
        "display(X_tr_tf_r.head(3))"
      ],
      "metadata": {
        "id": "VE_NBoquQsuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g3zcGQlfMCP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_transformation_3_explanations = \"\"\"\n",
        "**What I did**\n",
        "• **Numeric** → median imputation then **RobustScaler**, which scales by the IQR and is less sensitive to outliers\n",
        "  than StandardScaler.\n",
        "• **Categorical** → most-frequent imputation, then a **rare-category grouper** that merges very small levels (<1% of\n",
        "  train) into 'Other' before One-Hot Encoding (with unknowns ignored).\n",
        "\n",
        "**Why**\n",
        "• Retail-style features often contain extreme values; RobustScaler stabilises linear and distance-based models.\n",
        "• Grouping rare categories prevents very wide, sparse matrices and reduces variance in coefficients while keeping\n",
        "  interpretability (you still see a clean 'Other' bucket).\n",
        "\n",
        "**Outputs**\n",
        "• `X_tr_tf_r`, `X_va_tf_r`, `X_te_tf_r` — transformed DataFrames with feature names. Use alongside D.2 and D.3 to\n",
        "  pick the best preprocessing via validation performance.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RK-j77QRQsxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='data_transformation_3_explanations', value=data_transformation_3_explanations)"
      ],
      "metadata": {
        "id": "H0cpp9aCQs0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D.n Fixing \"\\<describe_issue_here\\>\"\n",
        "\n",
        "> You can add more cells related to data preparation in this section"
      ],
      "metadata": {
        "id": "fvT1FAD7MP6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generic data-prep fixes: drop constants/dupes, clip extreme outliers, align columns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Pick inputs: prefer transformed sets (D.4 → D.3 → D.2), then raw (D.1)\n",
        "def _first_df(*names):\n",
        "    for n in names:\n",
        "        obj = globals().get(n)\n",
        "        if isinstance(obj, pd.DataFrame) and not obj.empty:\n",
        "            return obj.copy(), n\n",
        "    return None, None\n",
        "\n",
        "Xtr, src_tr = _first_df(\"X_tr_tf_r\", \"X_tr_tf_q\", \"X_tr_tf\", \"X_tr\")\n",
        "Xva, src_va = _first_df(\"X_va_tf_r\", \"X_va_tf_q\", \"X_va_tf\", \"X_va\")\n",
        "Xte, src_te = _first_df(\"X_te_tf_r\", \"X_te_tf_q\", \"X_te_tf\", \"X_te\")\n",
        "\n",
        "assert Xtr is not None and Xva is not None and Xte is not None, \"Run D.1 and at least one of D.2–D.4.\"\n",
        "\n",
        "# 2) Drop constant columns (zero variance) on TRAIN, then apply same mask to VA/TEST\n",
        "const_cols = Xtr.columns[Xtr.nunique(dropna=False) <= 1].tolist()\n",
        "if const_cols:\n",
        "    Xtr = Xtr.drop(columns=const_cols)\n",
        "    Xva = Xva.drop(columns=[c for c in const_cols if c in Xva], errors=\"ignore\")\n",
        "    Xte = Xte.drop(columns=[c for c in const_cols if c in Xte], errors=\"ignore\")\n",
        "\n",
        "# 3) Drop duplicate columns w.r.t. TRAIN (exact duplicates)\n",
        "def _drop_dup_cols(df):\n",
        "    # transpose & drop_duplicates on rows -> back to columns\n",
        "    mask = ~df.T.duplicated()\n",
        "    return df.loc[:, mask], df.columns[~mask].tolist()\n",
        "\n",
        "Xtr, dup_cols = _drop_dup_cols(Xtr)\n",
        "if dup_cols:\n",
        "    Xva = Xva.drop(columns=[c for c in dup_cols if c in Xva], errors=\"ignore\")\n",
        "    Xte = Xte.drop(columns=[c for c in dup_cols if c in Xte], errors=\"ignore\")\n",
        "\n",
        "# 4) Robust outlier clipping on TRAIN numeric columns (winsorize to 1st–99th pct)\n",
        "num_cols = [c for c in Xtr.columns if pd.api.types.is_numeric_dtype(Xtr[c])]\n",
        "if num_cols:\n",
        "    q_low  = Xtr[num_cols].quantile(0.01)\n",
        "    q_high = Xtr[num_cols].quantile(0.99)\n",
        "    Xtr[num_cols] = Xtr[num_cols].clip(lower=q_low, upper=q_high, axis=1)\n",
        "    # Use the same cut points learned on TRAIN for VA/TEST\n",
        "    Xva[num_cols] = Xva[num_cols].clip(lower=q_low, upper=q_high, axis=1)\n",
        "    Xte[num_cols] = Xte[num_cols].clip(lower=q_low, upper=q_high, axis=1)\n",
        "\n",
        "# 5) Align columns across splits (in case upstream pipelines produced slight mismatches)\n",
        "common_cols = sorted(list(set(Xtr.columns) & set(Xva.columns) & set(Xte.columns)))\n",
        "Xtr_fix = Xtr[common_cols].copy()\n",
        "Xva_fix = Xva[common_cols].copy()\n",
        "Xte_fix = Xte[common_cols].copy()\n",
        "\n",
        "# 6) Expose results and a small report\n",
        "globals().update(dict(X_tr_fix=Xtr_fix, X_va_fix=Xva_fix, X_te_fix=Xte_fix))\n",
        "\n",
        "report = {\n",
        "    \"source_train\": src_tr, \"source_valid\": src_va, \"source_test\": src_te,\n",
        "    \"dropped_constants\": const_cols,\n",
        "    \"dropped_duplicates\": dup_cols,\n",
        "    \"num_cols_clipped\": len(num_cols),\n",
        "    \"final_shapes\": {\"train\": Xtr_fix.shape, \"valid\": Xva_fix.shape, \"test\": Xte_fix.shape},\n",
        "}\n",
        "print(report)\n",
        "display(Xtr_fix.head(3))"
      ],
      "metadata": {
        "id": "YqbadwJ4MP_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RxQajLlx_Ci2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_transformation_n_explanations = \"\"\"\n",
        "**Issue fixed:** Residual prep artefacts after earlier steps — constant/duplicate features, extreme values,\n",
        "and column mismatches across splits.\n",
        "\n",
        "**What I did**\n",
        "• Dropped **zero-variance** and **duplicate** columns (learned on train, applied to val/test).\n",
        "• Applied **robust winsorisation** (clip to 1st–99th percentiles) on numeric features using train cut-points to reduce\n",
        "  the influence of extreme outliers without discarding data.\n",
        "• **Aligned columns** to ensure train/validation/test have identical feature sets.\n",
        "\n",
        "**Why it matters**\n",
        "• Constant/duplicate columns add noise and can inflate model complexity without adding signal.\n",
        "• Extreme values can dominate loss/gradients and harm generalisation.\n",
        "• Misaligned columns silently break training/evaluation. These fixes make the dataset stable and reproducible.\n",
        "\n",
        "**Outputs**\n",
        "• Cleaned matrices: `X_tr_fix`, `X_va_fix`, `X_te_fix` with the same columns and shapes: {{\n",
        "    \"train\": {globals().get('X_tr_fix').shape if 'X_tr_fix' in globals() else 'n/a'},\n",
        "    \"valid\": {globals().get('X_va_fix').shape if 'X_va_fix' in globals() else 'n/a'},\n",
        "    \"test\":  {globals().get('X_te_fix').shape if 'X_te_fix' in globals() else 'n/a'}\n",
        "}}.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "V5dYUfTV_Dja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "print_tile(size=\"h3\", key='data_transformation_n_explanations', value=data_transformation_n_explanations)"
      ],
      "metadata": {
        "id": "Wi0yK5kg_D8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bridge variables for the Save Datasets cell\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def _pick_df(*candidates):\n",
        "    \"\"\"Return the first non-empty DataFrame among the given global names.\"\"\"\n",
        "    for name in candidates:\n",
        "        obj = globals().get(name)\n",
        "        if isinstance(obj, pd.DataFrame) and not obj.empty:\n",
        "            return obj.copy()\n",
        "    return None\n",
        "\n",
        "def _ensure_df(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    if isinstance(x, pd.DataFrame):\n",
        "        return x.copy()\n",
        "    # numpy array -> DataFrame\n",
        "    try:\n",
        "        return pd.DataFrame(x).copy()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _pick_y(*names):\n",
        "    \"\"\"Pick a y Series if available; else return None (unsupervised).\"\"\"\n",
        "    for n in names:\n",
        "        obj = globals().get(n)\n",
        "        if obj is not None:\n",
        "            # DataFrame with one column -> Series\n",
        "            if isinstance(obj, pd.DataFrame) and obj.shape[1] == 1:\n",
        "                return obj.iloc[:, 0].copy()\n",
        "            # Series ok\n",
        "            if isinstance(obj, (pd.Series, pd.Index)):\n",
        "                return pd.Series(obj).copy()\n",
        "    return None\n",
        "\n",
        "# Prefer the most prepared matrices; fall back to raw\n",
        "X_train = _ensure_df(_pick_df(\"X_tr_fix\", \"X_tr_tf_r\", \"X_tr_tf_q\", \"X_tr_tf\", \"X_tr\"))\n",
        "X_val   = _ensure_df(_pick_df(\"X_va_fix\", \"X_va_tf_r\", \"X_va_tf_q\", \"X_va_tf\", \"X_va\"))\n",
        "X_test  = _ensure_df(_pick_df(\"X_te_fix\", \"X_te_tf_r\", \"X_te_tf_q\", \"X_te_tf\", \"X_te\"))\n",
        "\n",
        "# Targets (may be None for unsupervised datasets)\n",
        "y_train = _pick_y(\"y_tr\")\n",
        "y_val   = _pick_y(\"y_va\")\n",
        "y_test  = _pick_y(\"y_te\")\n",
        "\n",
        "# If no targets exist (unsupervised), create empty Series so the save cell still runs\n",
        "target_name = globals().get(\"y_col\", \"target\")\n",
        "if y_train is None:\n",
        "    y_train = pd.Series([np.nan]*len(X_train), name=target_name)\n",
        "if y_val is None:\n",
        "    y_val = pd.Series([np.nan]*len(X_val), name=target_name)\n",
        "if y_test is None:\n",
        "    y_test = pd.Series([np.nan]*len(X_test), name=target_name)\n",
        "\n",
        "# Final sanity prints (optional)\n",
        "print(\"Ready to save:\",\n",
        "      {\"X_train\": X_train.shape, \"X_val\": X_val.shape, \"X_test\": X_test.shape,\n",
        "       \"y_train_len\": len(y_train), \"y_val_len\": len(y_val), \"y_test_len\": len(y_test)})"
      ],
      "metadata": {
        "id": "luWMYQnrwkPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## E. Save Datasets\n",
        "\n",
        "> Do not change this code"
      ],
      "metadata": {
        "id": "s0CJolMhdmLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THE CODE IN THIS CELL\n",
        "\n",
        "try:\n",
        "  X_train.to_csv(at.folder_path / 'X_train.csv', index=False)\n",
        "  y_train.to_csv(at.folder_path / 'y_train.csv', index=False)\n",
        "\n",
        "  X_val.to_csv(at.folder_path / 'X_val.csv', index=False)\n",
        "  y_val.to_csv(at.folder_path / 'y_val.csv', index=False)\n",
        "\n",
        "  X_test.to_csv(at.folder_path / 'X_test.csv', index=False)\n",
        "  y_test.to_csv(at.folder_path / 'y_test.csv', index=False)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "hwIgKOsGdrf3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}